{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zswsuw35uV1Q"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8AYewsFte9m"
      },
      "source": [
        "# Load pretrained word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h8mHA5ItdNH",
        "outputId": "202036ed-966c-4afc-ff81-c743513c6ab9"
      },
      "source": [
        "import pickle\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "path = '/content/drive/MyDrive/Colab Notebooks'\r\n",
        "embeddings_dict = pickle.load(open(f'{path}/embeddings_dict.pkl', 'rb'))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffa21u-BtjFM"
      },
      "source": [
        "# Define LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb6dD99CWkGX"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "else:\r\n",
        "    device = torch.device(\"cpu\")\r\n",
        "\r\n",
        "def get_weights(target_vocab, embeddings_dict, embedding_dim=100):\r\n",
        "    matrix_len = len(target_vocab)\r\n",
        "    weights_matrix = np.zeros((matrix_len, embedding_dim))\r\n",
        "    words_found = 0\r\n",
        "\r\n",
        "    for i, word in enumerate(target_vocab):\r\n",
        "        try: \r\n",
        "            weights_matrix[i] = embeddings_dict[word]\r\n",
        "            words_found += 1\r\n",
        "        except KeyError:\r\n",
        "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))  \r\n",
        "\r\n",
        "    print(\"Fraction of vocab words found in word embedding: \", words_found/matrix_len)\r\n",
        "    return torch.tensor(weights_matrix)\r\n",
        "\r\n",
        "def create_emb_layer(weights_matrix, trainable=False):\r\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape\r\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\r\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix})\r\n",
        "    if not trainable:\r\n",
        "        emb_layer.weight.requires_grad = False\r\n",
        "      \r\n",
        "    return emb_layer, num_embeddings, embedding_dim  \r\n",
        "\r\n",
        "class SentimentNet(nn.Module):\r\n",
        "    def __init__(self, target_vocab, embeddings_dict, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\r\n",
        "        super(SentimentNet, self).__init__()\r\n",
        "        self.output_size = output_size\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "\r\n",
        "        # Learn new word embedding\r\n",
        "        # vocab_size = len(target_vocab)\r\n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "\r\n",
        "        # Use pretrained word embedding\r\n",
        "        weights_matrix = get_weights(target_vocab, embeddings_dict, embedding_dim)\r\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, False)\r\n",
        "        \r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\r\n",
        "        self.dropout = nn.Dropout(drop_prob)\r\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\r\n",
        "        self.sigmoid = nn.Sigmoid()\r\n",
        "        \r\n",
        "    def forward(self, x, hidden):\r\n",
        "        batch_size = x.size(0)\r\n",
        "        x = x.long()\r\n",
        "        embeds = self.embedding(x)\r\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\r\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\r\n",
        "        \r\n",
        "        out = self.dropout(lstm_out)\r\n",
        "        out = self.fc(out)\r\n",
        "        out = self.sigmoid(out)\r\n",
        "        \r\n",
        "        out = out.view(batch_size, -1)\r\n",
        "        out = out[:,-1]\r\n",
        "        return out, hidden\r\n",
        "    \r\n",
        "    def init_hidden(self, batch_size):\r\n",
        "        weight = next(self.parameters()).data\r\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\r\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\r\n",
        "        return hidden"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMD5MO7bstul"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9irSayVjR6q"
      },
      "source": [
        "# Pad/truncate sentence to fixed length\r\n",
        "# https://arxiv.org/abs/1903.07288 pad zeros in front\r\n",
        "def pad_input(sentences, seq_len=200):\r\n",
        "    features = np.zeros((len(sentences), seq_len), dtype=int)\r\n",
        "    for i, tokens in enumerate(sentences):\r\n",
        "        if len(tokens) > seq_len:\r\n",
        "            features[i] = np.array(tokens)[:seq_len]\r\n",
        "        elif len(tokens) > 0:\r\n",
        "            features[i, -len(tokens):] = np.array(tokens)\r\n",
        "    return features\r\n",
        "\r\n",
        "def preprocess_input(X, y, seq_len=100):\r\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\r\n",
        "\r\n",
        "    words = Counter() \r\n",
        "\r\n",
        "    for i, sentence in enumerate(X_train):\r\n",
        "        # The sentences will be stored as a list of words/tokens\r\n",
        "        X_train[i] = []\r\n",
        "        for word in nltk.word_tokenize(sentence):  # Tokenizing the words\r\n",
        "            words.update([word.lower()])  # Converting all the words to lowercase\r\n",
        "            X_train[i].append(word)\r\n",
        "    \r\n",
        "    # # Removing the words that only appear once\r\n",
        "    words = {k:v for k,v in words.items() if v>1}\r\n",
        "    # # Sorting the words according to the number of appearances, with the most common word being first\r\n",
        "    words = sorted(words, key=words.get, reverse=True)\r\n",
        "    # Adding padding and unknown to our vocabulary so that they will be assigned an index\r\n",
        "    words = ['_PAD'] + words\r\n",
        "\r\n",
        "    # Dictionaries to store the word to index mappings and vice versa\r\n",
        "    word2idx = {o:i for i,o in enumerate(words)}\r\n",
        "    idx2word = {i:o for i,o in enumerate(words)}\r\n",
        "\r\n",
        "    for i, sentence in enumerate(X_train):\r\n",
        "        X_train[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\r\n",
        "\r\n",
        "    for i, sentence in enumerate(X_val):\r\n",
        "        X_val[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]\r\n",
        "\r\n",
        "    for i, sentence in enumerate(X_test):\r\n",
        "        X_test[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]\r\n",
        "\r\n",
        "    train_sentences = pad_input(X_train, seq_len)\r\n",
        "    val_sentences = pad_input(X_val, seq_len)\r\n",
        "    test_sentences = pad_input(X_test, seq_len)\r\n",
        "\r\n",
        "    return train_sentences, val_sentences, test_sentences, y_train, y_val, y_test, words, idx2word"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qs3MAd1szWS"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pluu00U1PXCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b13f0a-e05f-4c14-968a-44fd09d13f0c"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from scipy.sparse import csr_matrix\r\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "import math\r\n",
        "from collections import Counter\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk import word_tokenize  \r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\r\n",
        "    train = pd.read_csv(url)\r\n",
        "    X = np.array(train['comment_text'])\r\n",
        "    y = np.array(train['toxic_label'])\r\n",
        "\r\n",
        "    size = 5000\r\n",
        "    X = X[:size]\r\n",
        "    y = y[:size]\r\n",
        "\r\n",
        "    train_sentences, val_sentences, test_sentences, \\\r\n",
        "    y_train, y_val, y_test, vocab, idx2word = preprocess_input(X, y)\r\n",
        "\r\n",
        "    train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(y_train))\r\n",
        "    val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(y_val))\r\n",
        "    test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(y_test))\r\n",
        "\r\n",
        "    batch_size = 200\r\n",
        "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
        "    val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
        "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
        "\r\n",
        "    output_size = 1\r\n",
        "    embedding_dim = 100\r\n",
        "    hidden_dim = 256\r\n",
        "    n_layers = 2\r\n",
        "    vocab_size = len(vocab)\r\n",
        "\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        device = torch.device(\"cuda\")\r\n",
        "    else:\r\n",
        "        device = torch.device(\"cpu\")\r\n",
        "    print(device)\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    model = SentimentNet(vocab, embeddings_dict, output_size, embedding_dim, hidden_dim, n_layers)\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "    lr=0.01\r\n",
        "    criterion = nn.BCELoss()\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "    epochs = 10\r\n",
        "    counter = 0\r\n",
        "    print_every = int(0.5 * len(y_train) / batch_size)\r\n",
        "    clip = 5\r\n",
        "    valid_loss_min = np.Inf\r\n",
        "\r\n",
        "    # Set seed\r\n",
        "    torch.manual_seed(1)\r\n",
        "\r\n",
        "    model.train()\r\n",
        "    for i in range(epochs):\r\n",
        "        h = model.init_hidden(batch_size)\r\n",
        "        \r\n",
        "        for inputs, labels in train_loader:\r\n",
        "            counter += 1\r\n",
        "            h = tuple([e.data for e in h])\r\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "            model.zero_grad()\r\n",
        "            output, h = model(inputs, h)\r\n",
        "            loss = criterion(output.squeeze(), labels.float())\r\n",
        "            loss.backward()\r\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "            optimizer.step()\r\n",
        "            \r\n",
        "            if counter % print_every == 0:\r\n",
        "                val_h = model.init_hidden(batch_size)\r\n",
        "                val_losses = []\r\n",
        "                model.eval()\r\n",
        "                for val_input, val_label in val_loader:\r\n",
        "                    val_h = tuple([each.data for each in val_h])\r\n",
        "                    val_input, val_label = val_input.to(device), val_label.to(device)\r\n",
        "                    out, val_h = model(val_input, val_h)\r\n",
        "                    val_loss = criterion(out.squeeze(), val_label.float())\r\n",
        "                    val_losses.append(val_loss.item())\r\n",
        "                    \r\n",
        "                model.train()\r\n",
        "                print(\"Epoch: {}/{}...\".format(i+1, epochs),\r\n",
        "                      \"Step: {}...\".format(counter),\r\n",
        "                      \"Loss: {:.6f}...\".format(loss.item()),\r\n",
        "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\r\n",
        "                if np.mean(val_losses) <= valid_loss_min:\r\n",
        "                    # torch.save(model.state_dict(), './state_dict.pt')\r\n",
        "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\r\n",
        "                    valid_loss_min = np.mean(val_losses)\r\n",
        "\r\n",
        "    test_losses = []\r\n",
        "    h = model.init_hidden(batch_size)\r\n",
        "    test_inputs = np.array([])\r\n",
        "    outputs = np.array([])\r\n",
        "    test_labels = np.array([])\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        for inputs, labels in test_loader:\r\n",
        "            h = tuple([each.data for each in h])\r\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "            output, h = model(inputs, h)\r\n",
        "            test_loss = criterion(output.squeeze(), labels.float())\r\n",
        "            test_losses.append(test_loss.item())\r\n",
        "            pred = torch.round(output.squeeze())\r\n",
        "\r\n",
        "            if len(test_inputs) == 0:\r\n",
        "                test_inputs = inputs.cpu().numpy()\r\n",
        "            else:\r\n",
        "                test_inputs = np.concatenate([test_inputs, inputs.cpu().numpy()])\r\n",
        "            outputs = np.concatenate([outputs, pred.cpu().numpy()])\r\n",
        "            test_labels = np.concatenate([test_labels, labels.cpu().numpy()])\r\n",
        "        \r\n",
        "        f1score = f1_score(np.array(test_labels), outputs)\r\n",
        "        recall = recall_score(np.array(test_labels), outputs)\r\n",
        "        precision = precision_score(np.array(test_labels), outputs)\r\n",
        "        accuracy = accuracy_score(np.array(test_labels), outputs)\r\n",
        "        print(f\"Test F1 score: {f1score}\")\r\n",
        "        print(f\"Test recall: {recall}\")\r\n",
        "        print(f\"Test precision: {precision}\")\r\n",
        "        print(f\"Test accuracy: {accuracy}\")\r\n",
        "        print(\"##############################################################\")\r\n",
        "        # Print some wrong predictions\r\n",
        "        for i, input in enumerate(test_inputs[:100]):\r\n",
        "            if outputs[i] != test_labels[i]:\r\n",
        "              input_no_pad = input[input != 0]\r\n",
        "              print(f\"Label: {test_labels[i]}, Prediction: {outputs[i]}\")\r\n",
        "              print(\" \".join([idx2word[i] for i in input_no_pad]))\r\n",
        "              print()\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    main()\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "cuda\n",
            "Fraction of vocab words found in word embedding:  0.8813559322033898\n",
            "Epoch: 1/10... Step: 10... Loss: 0.337643... Val Loss: 0.308498\n",
            "Validation loss decreased (inf --> 0.308498).  Saving model ...\n",
            "Epoch: 1/10... Step: 20... Loss: 0.353720... Val Loss: 0.308544\n",
            "Epoch: 2/10... Step: 30... Loss: 0.344862... Val Loss: 0.296293\n",
            "Validation loss decreased (0.308498 --> 0.296293).  Saving model ...\n",
            "Epoch: 2/10... Step: 40... Loss: 0.254464... Val Loss: 0.283745\n",
            "Validation loss decreased (0.296293 --> 0.283745).  Saving model ...\n",
            "Epoch: 3/10... Step: 50... Loss: 0.265329... Val Loss: 0.242124\n",
            "Validation loss decreased (0.283745 --> 0.242124).  Saving model ...\n",
            "Epoch: 3/10... Step: 60... Loss: 0.232867... Val Loss: 0.262773\n",
            "Epoch: 4/10... Step: 70... Loss: 0.230754... Val Loss: 0.212570\n",
            "Validation loss decreased (0.242124 --> 0.212570).  Saving model ...\n",
            "Epoch: 4/10... Step: 80... Loss: 0.162302... Val Loss: 0.224807\n",
            "Epoch: 5/10... Step: 90... Loss: 0.287199... Val Loss: 0.231733\n",
            "Epoch: 5/10... Step: 100... Loss: 0.256775... Val Loss: 0.185076\n",
            "Validation loss decreased (0.212570 --> 0.185076).  Saving model ...\n",
            "Epoch: 6/10... Step: 110... Loss: 0.130256... Val Loss: 0.282876\n",
            "Epoch: 6/10... Step: 120... Loss: 0.248265... Val Loss: 0.199494\n",
            "Epoch: 7/10... Step: 130... Loss: 0.222439... Val Loss: 0.217606\n",
            "Epoch: 7/10... Step: 140... Loss: 0.242693... Val Loss: 0.199493\n",
            "Epoch: 8/10... Step: 150... Loss: 0.222098... Val Loss: 0.155846\n",
            "Validation loss decreased (0.185076 --> 0.155846).  Saving model ...\n",
            "Epoch: 8/10... Step: 160... Loss: 0.160300... Val Loss: 0.171281\n",
            "Epoch: 9/10... Step: 170... Loss: 0.173130... Val Loss: 0.214599\n",
            "Epoch: 9/10... Step: 180... Loss: 0.232971... Val Loss: 0.166316\n",
            "Epoch: 10/10... Step: 190... Loss: 0.157528... Val Loss: 0.190587\n",
            "Epoch: 10/10... Step: 200... Loss: 0.066707... Val Loss: 0.175269\n",
            "Test F1 score: 0.679245283018868\n",
            "Test recall: 0.5294117647058824\n",
            "Test precision: 0.9473684210526315\n",
            "Test accuracy: 0.9575\n",
            "##############################################################\n",
            "Label: 1.0, Prediction: 0.0\n",
            "stop ban me ! you did it before !\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` something anonymous editing has given rise to vandalism . pass to all vandalism is good . vandalism does not come from a desire to cause harm . vandals do their thing all over wiki , , from frustration and hatred , because `` '' editors '' '' who have no skills , no degrees , no expertise , usually no names , have done multiple reverts and edits of the work of others . vandalism is a good and natural response to ( as in get a life ) who do edits\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zswsuw35uV1Q"
      },
      "source": [
        "# Train Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7XqXLaruOzh",
        "outputId": "0070b795-51a6-4c81-82d9-8e8d0a70f043"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from scipy.sparse import csr_matrix\r\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "import math\r\n",
        "from collections import Counter\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk import word_tokenize  \r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\r\n",
        "    train = pd.read_csv(url)\r\n",
        "    X = np.array(train['comment_text'])\r\n",
        "    y = np.array(train['toxic_label'])\r\n",
        "\r\n",
        "    size = 10000\r\n",
        "    X = X[:size]\r\n",
        "    y = y[:size]\r\n",
        "\r\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\r\n",
        "\r\n",
        "    model = LogisticRegression(random_state=0, solver='sag', max_iter=200)\r\n",
        "    vectorizer = TfidfVectorizer()\r\n",
        "    X_train_tf_idf_matrix = vectorizer.fit_transform(X_train)\r\n",
        "    model.fit( X_train_tf_idf_matrix, y_train)\r\n",
        "\r\n",
        "    # test your model\r\n",
        "    vectorizer_val = TfidfVectorizer(vocabulary=vectorizer.get_feature_names())\r\n",
        "    # vectorizer_val = TfidfVectorizer(vocabulary=vocab)\r\n",
        "    X_val_tf_idf_matrix = vectorizer_val.fit_transform(X_val)\r\n",
        "\r\n",
        "    y_pred = model.predict(X_val_tf_idf_matrix)\r\n",
        "    score = f1_score(y_val, y_pred, average='macro')\r\n",
        "    acc = accuracy_score(y_val, y_pred)\r\n",
        "    print('F1 score on validation = {}'.format(score))\r\n",
        "    print('accuracy = {}'.format(acc))\r\n",
        " \r\n",
        "if __name__ == \"__main__\":\r\n",
        "    main()\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "F1 score on validation = 0.7162126068376068\n",
            "accuracy = 0.932\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}