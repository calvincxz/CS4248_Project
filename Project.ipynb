{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ffa21u-BtjFM",
        "i6jR6yjifjbt"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78dGhMtdvq81"
      },
      "source": [
        "# Data scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLlRx3wc3Qww",
        "outputId": "14f50af8-f78a-49cc-e6f3-4fb6121a4302"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jANQNZG8nUxR"
      },
      "source": [
        "import json\n",
        "\n",
        "subdir1 = \"CS4248 Project/ntuconfessions_data.json\"\n",
        "subdir2 = \"CS4248 Project/ntu.json\"\n",
        "\n",
        "def load_file(subdir):\n",
        "    path = f'/content/drive/MyDrive/{subdir}'\n",
        "    data = json.load(open(f'{path}', 'rb'))\n",
        "    return data\n",
        "    \n",
        "data1 = load_file(subdir1)\n",
        "data2 = load_file(subdir2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6YDQu2DnyLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fc0009-6638-4358-e89c-ecf882a0600d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_data(data):\n",
        "    posts = data[0][\"posts\"]\n",
        "    visited_urls = set()\n",
        "    post_contents = []\n",
        "    post_comments = []\n",
        "    for post in posts:\n",
        "        if post[\"postUrl\"] in visited_urls:\n",
        "            continue\n",
        "            \n",
        "        visited_urls.add(post[\"postUrl\"])\n",
        "        post_text = post[\"postText\"]\n",
        "        # remove hyperlink at end of post\n",
        "        # truncated_text = post_text[:-53] # for nus\n",
        "        truncated_text = post_text[:-47] # for ntu\n",
        "        # truncated_text = post_text\n",
        "        post_contents.append(truncated_text)\n",
        "\n",
        "        for comment in post[\"postComments\"][\"comments\"]:\n",
        "            comment_text = comment[\"text\"]\n",
        "            post_comments.append(comment_text)\n",
        "    return post_contents, post_comments\n",
        "\n",
        "x1, x2 = process_data(data1)\n",
        "x3, x4 = process_data(data2)\n",
        "print(len(x3), len(x4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "652 3039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6OkZKim3iN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9e83ad-95c4-4b4d-ec85-ce6b839aeb49"
      },
      "source": [
        "post_contents = x1 + x2\n",
        "post_comments = x3 + x4\n",
        "print(len(post_contents), len(post_comments))\n",
        "\n",
        "df1 = pd.DataFrame (post_contents,columns=['post_contents'])\n",
        "df2 = pd.DataFrame (post_comments,columns=['post_comments'])\n",
        "\n",
        "df1.to_csv(r'ntu_posts.csv', index = False, header=True)\n",
        "df2.to_csv(r'ntu_comments.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "482 3691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8AYewsFte9m"
      },
      "source": [
        "# Load pretrained word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h8mHA5ItdNH",
        "outputId": "c5cdc368-d2b1-4377-a90d-ea511516b76a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW66B2Pdropx"
      },
      "source": [
        "import pickle\n",
        "import json\n",
        "\n",
        "# Define your own subdirectory \n",
        "subdir = \"CS4248 Project/Data/\"\n",
        "path = f'/content/drive/MyDrive/{subdir}'\n",
        "\n",
        "twitter_embedding_name = \"embeddings_dict.pkl\"\n",
        "wikipedia_embedding_name = \"embeddings_dict_wikipedia_100d.pkl\"\n",
        "finetune_embedding_name = \"sms_glove.pkl\"\n",
        "finetune_embedding_name_ntu = \"ntu_comment_glove.pkl\"\n",
        "\n",
        "\n",
        "twitter_embeddings_dict = pickle.load(open(f'{path}{twitter_embedding_name}', 'rb'))\n",
        "# wikipedia_embeddings_dict = pickle.load(open(f'{path}{wikipedia_embedding_name}', 'rb'))\n",
        "\n",
        "# update embedding\n",
        "# combined_embeddings_dict = {}\n",
        "new_embeddings_dict = pickle.load(open(f'{path}{finetune_embedding_name}', 'rb'))\n",
        "new_embeddings_dict.update(twitter_embeddings_dict)\n",
        "\n",
        "new_embeddings_dict_2 = pickle.load(open(f'{path}{finetune_embedding_name_ntu}', 'rb'))\n",
        "new_embeddings_dict_2.update(twitter_embeddings_dict)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffa21u-BtjFM"
      },
      "source": [
        "# Define LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb6dD99CWkGX"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import textwrap\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "def get_weights(target_vocab, embeddings_dict, embedding_dim=100):\n",
        "    matrix_len = len(target_vocab)\n",
        "    weights_matrix = np.zeros((matrix_len, embedding_dim))\n",
        "    words_found = 0\n",
        "    oov_words = []\n",
        "\n",
        "    for i, word in enumerate(target_vocab):\n",
        "        try: \n",
        "            # print(embeddings_dict[word].shape)\n",
        "            weights_matrix[i] = embeddings_dict[word]\n",
        "            words_found += 1\n",
        "        except KeyError:\n",
        "            # oov_words.append(word)\n",
        "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
        "        except:\n",
        "            pass  \n",
        "\n",
        "    return torch.tensor(weights_matrix)\n",
        "\n",
        "def create_emb_layer(weights_matrix, trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "    # set trainable for weights\n",
        "    emb_layer.weight.requires_grad = trainable\n",
        "      \n",
        "    return emb_layer, num_embeddings, embedding_dim  \n",
        "\n",
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, target_vocab, embeddings_dict, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Learn new word embedding\n",
        "        # vocab_size = len(target_vocab)\n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Use pretrained word embedding\n",
        "        weights_matrix = get_weights(target_vocab, embeddings_dict, embedding_dim)\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, trainable=True)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        self.lstm.flatten_parameters() \n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        # out = self.dropout(lstm_out)\n",
        "        out = self.fc1(lstm_out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMD5MO7bstul"
      },
      "source": [
        "# Choice 1: Preprocessing using only training set vocab as embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9irSayVjR6q"
      },
      "source": [
        "# Pad/truncate sentence to fixed length\n",
        "# https://arxiv.org/abs/1903.07288 pad zeros in front\n",
        "def pad_input(sentences, seq_len=100):\n",
        "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
        "    for i, tokens in enumerate(sentences):\n",
        "        if len(tokens) > seq_len:\n",
        "            features[i] = np.array(tokens)[:seq_len]\n",
        "        elif len(tokens) > 0:\n",
        "            features[i, -len(tokens):] = np.array(tokens)\n",
        "    return features\n",
        "\n",
        "def preprocess_input(X, y, seq_len=100, X_test_local=[], y_test_local=[], embeddings=None, replace_oov=False):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\n",
        "\n",
        "    \n",
        "    if len(X_test_local) > 0:\n",
        "        X_test, X_val, y_test, y_val = train_test_split(X_test_local, y_test_local, test_size=0.5, random_state=0)\n",
        "        # X_test = X_test_local\n",
        "        # y_test = y_test_local\n",
        "\n",
        "    # print(len(X_test))\n",
        "    words = Counter() \n",
        "    count = 0\n",
        "    for i, sentence in enumerate(X_train):\n",
        "        # The sentences will be stored as a list of words/tokens\n",
        "        X_train[i] = []\n",
        "        for word in nltk.word_tokenize(sentence):  \n",
        "            processed_word = word.lower()\n",
        "            words.update([processed_word]) \n",
        "            X_train[i].append(processed_word)\n",
        "    \n",
        "    words = {k:v for k,v in words.items() if v > 1}\n",
        "    words = sorted(words, key=words.get, reverse=True)\n",
        "    # Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
        "    words = ['_PAD', '_UNKNOWN'] + words\n",
        "\n",
        "    # Dictionaries to store the word to index mappings and vice versa\n",
        "    word2idx = {o:i for i,o in enumerate(words)}\n",
        "    idx2word = {i:o for i,o in enumerate(words)}\n",
        "\n",
        "    for i, sentence in enumerate(X_train):\n",
        "        X_train[i] = [word2idx[word] if word in word2idx else 1 for word in sentence]\n",
        "\n",
        "    for i, sentence in enumerate(X_val):\n",
        "        X_val[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\n",
        "\n",
        "    oov = set()\n",
        "    for i, sentence in enumerate(X_test):\n",
        "        X_test[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\n",
        "        \n",
        "        for word in nltk.word_tokenize(sentence):\n",
        "            if word.lower() not in word2idx:\n",
        "                oov.add(word)\n",
        "    oov = list(oov)\n",
        "\n",
        "    print(\"----------------Training input statistics----------------\")\n",
        "    print(\"Preview of first 50 OOV words in test data: \", oov[:50])\n",
        "    print(\"Total number of OOV words in test data: \", len(oov))\n",
        "    print(\"Max tokens in training data: \", np.max([len(x) for x in X_train]))\n",
        "    # print(\"Min tokens: \", np.min([len(x) for x in X_train]))\n",
        "    print(\"Average tokens: \", np.mean([len(x) for x in X_train]))\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "    train_sentences = pad_input(X_train, seq_len)\n",
        "    val_sentences = pad_input(X_val, seq_len)\n",
        "    test_sentences = pad_input(X_test, seq_len)\n",
        "\n",
        "    return train_sentences, val_sentences, test_sentences, y_train, y_val, y_test, words, idx2word"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA8otp-Xvroi"
      },
      "source": [
        "# Choice 2: Preprocessing using entire embedding vocab as embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33s-7ZPgvpTA"
      },
      "source": [
        "# Pad/truncate sentence to fixed length\n",
        "# https://arxiv.org/abs/1903.07288 pad zeros in front\n",
        "def pad_input(sentences, seq_len=100):\n",
        "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
        "    for i, tokens in enumerate(sentences):\n",
        "        if len(tokens) >= seq_len:\n",
        "            features[i] = np.array(tokens)[:seq_len]\n",
        "        elif len(tokens) > 0:\n",
        "            features[i, -len(tokens):] = np.array(tokens)\n",
        "    return features\n",
        "\n",
        "from nltk.metrics import edit_distance\n",
        "import re\n",
        "def get_closest_word(word, dictionary):\n",
        "    min = np.inf\n",
        "    closest = \"_UNKNOWN\"\n",
        "    word = re.sub(r'[^\\w\\s]', '', word)\n",
        "    print(word)\n",
        "    for s in dictionary:\n",
        "        if s == word:\n",
        "            return s\n",
        "\n",
        "        if abs(len(s) - len(word)) > 2:\n",
        "            continue\n",
        "        dist = edit_distance(word, s)\n",
        "        # optimizations\n",
        "        if dist < 3:\n",
        "            return s\n",
        "\n",
        "        if dist < min:\n",
        "            closest = s\n",
        "            min = dist\n",
        "    return closest\n",
        "\n",
        "def preprocess_input(X, y, seq_len=100, X_test_local=[], y_test_local=[], embeddings=None, replace_oov=False):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\n",
        "\n",
        "    \n",
        "    if len(X_test_local) > 0:\n",
        "        X_test, X_val, y_test, y_val = train_test_split(X_test_local, y_test_local, test_size=0.5, random_state=0)\n",
        "        # X_test = X_test_local\n",
        "        # y_test = y_test_local\n",
        "\n",
        "    # print(len(X_test))\n",
        "    words = list(embeddings.keys())\n",
        "    print(\"Embedding size: \", len(words))\n",
        "    \n",
        "    # words = {k:v for k,v in words.items() if v > 1}\n",
        "    # words = sorted(words, key=words.get, reverse=True)\n",
        "    # Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
        "    words = ['_PAD', '_UNKNOWN'] + words\n",
        "\n",
        "    # Dictionaries to store the word to index mappings and vice versa\n",
        "    word2idx = {o:i for i,o in enumerate(words)}\n",
        "    idx2word = {i:o for i,o in enumerate(words)}\n",
        "\n",
        "    for i, sentence in enumerate(X_train):\n",
        "        X_train[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\n",
        "\n",
        "    oov = set()\n",
        "    word2closestword = {}\n",
        "    for i, sentence in enumerate(np.concatenate((X_val, X_test))):\n",
        "        # X_test[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\n",
        "        for word in nltk.word_tokenize(sentence):\n",
        "            w = word.lower()\n",
        "            if w not in word2idx and w not in word2closestword.keys():\n",
        "                oov.add(w)\n",
        "                if replace_oov:\n",
        "                    word2closestword[w] = get_closest_word(w, words)\n",
        "                # print(\"Processed OOV: \", len(list(oov)))\n",
        "\n",
        "    for i, sentence in enumerate(X_val):\n",
        "        if replace_oov:\n",
        "            X_val[i] = [word2idx[word.lower()] if word.lower() in word2idx else word2idx[word2closestword[word.lower()]]\\\n",
        "                      for word in nltk.word_tokenize(sentence)]\n",
        "        else:\n",
        "            X_val[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\n",
        "\n",
        "    for i, sentence in enumerate(X_test):\n",
        "        if replace_oov:\n",
        "            X_test[i] = [word2idx[word.lower()] if word.lower() in word2idx else word2idx[word2closestword[word.lower()]]\\\n",
        "                      for word in nltk.word_tokenize(sentence)]\n",
        "        else:\n",
        "            X_test[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\n",
        "\n",
        "    oov = list(oov)\n",
        "    print(\"----------------Training input statistics----------------\")\n",
        "    print(\"Substituted words: \", word2closestword)\n",
        "    print(\"Preview of first 50 OOV words in test data: \", oov[:50])\n",
        "    print(\"Total number of OOV words in test data: \", len(oov))\n",
        "    print(\"Max tokens in training data: \", np.max([len(x) for x in X_train]))\n",
        "    # print(\"Min tokens: \", np.min([len(x) for x in X_train]))\n",
        "    print(\"Average tokens: \", np.mean([len(x) for x in X_train]))\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "    train_sentences = pad_input(X_train, seq_len)\n",
        "    val_sentences = pad_input(X_val, seq_len)\n",
        "    test_sentences = pad_input(X_test, seq_len)\n",
        "\n",
        "    return train_sentences, val_sentences, test_sentences, y_train, y_val, y_test, words, idx2word\n",
        "    \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr8HVhxLilwY",
        "outputId": "f3bf4d66-652c-4d44-a2d4-b37329f2fd1f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize  \n",
        "\n",
        "url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\n",
        "train = pd.read_csv(url)\n",
        "X = np.array(train['comment_text'])\n",
        "y = np.array(train['toxic_label'])\n",
        "\n",
        "# reduce size for faster training\n",
        "size = X.shape[0]\n",
        "# size = 40000\n",
        "\n",
        "X = X[:size]\n",
        "y = y[:size]\n",
        "X_test_local = []\n",
        "y_test_local = []\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/local_data_301.csv\"\n",
        "test_local = pd.read_csv(url)\n",
        "X_test_local = np.array(test_local['Comment'])\n",
        "y_test_local = np.array(test_local['Insult'])\n",
        "\n",
        "train_sentences, val_sentences, test_sentences, y_train, y_val, y_test, vocab, idx2word \\\n",
        "= preprocess_input(X, y, seq_len=200, X_test_local=X_test_local, y_test_local=y_test_local)\n",
        "\n",
        "\n",
        "# train_and_evaluate_model(embeddings_dict=wikipedia_embeddings_dict, X_test_local=X_test_local, y_test_local=y_test_local)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Embedding size:  1193514\n",
            "----------------Training input statistics----------------\n",
            "Preview of first 50 OOV words in test data:  ['Groovelife', '...', 'deglove', 'ahmah', 'kwailan', 'limpeh', 'sinkie', 'la..china', \"y'all\", 'uplorry', '...', '...', 'Covidiots', 'dormitories', '100/day', '600', 'leg..', '80', '20', '20', 'dormitories', 'dormitories', 'expenses…', 'Pappies', 'PG/MG', 'peepur', 'COVID-19', '...', 'northing', '...', 'extramiles', '..', '...', 'pedestral', '1', 'laosai', 'Kbkb', 'covid', 'satki', 'rubbish.Everywhere', 'Covid', 'mah..', 'Sporeans', 'satki', '2k', '5', 'ourselves..', 'again..', 'sh*t', 'sinkie']\n",
            "Total number of OOV words in test data:  109\n",
            "Max tokens in training data:  4948\n",
            "Average tokens:  75.06390318054257\n",
            "----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6jR6yjifjbt"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "xzJddAwQfhSg",
        "outputId": "4482a206-797d-4663-de08-3ec72e95bfc7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize  \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\n",
        "train = pd.read_csv(url)\n",
        "X = np.array(train['comment_text'])\n",
        "y = np.array(train['toxic_label'])\n",
        "\n",
        "for i, sentence in enumerate(X):\n",
        "    X[i] = []\n",
        "    for word in nltk.word_tokenize(sentence): \n",
        "        X[i].append(word)\n",
        "\n",
        "x1 = [\"Toxic\", \"Non-toxic\"]\n",
        "y1 = [sum(y), len(y)-sum(y)]\n",
        "\n",
        "fig = plt.figure(figsize =(4,6))\n",
        "plt.bar(x1, y1, label=\"Blue Bar\", )\n",
        "plt.plot()\n",
        "\n",
        "plt.xlabel(\"Comment Type\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.title(\"Toxic/Non-toxic Distribution\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "n = [len(x) for x in X]\n",
        "m = [m for m in range(len(n))]\n",
        "\n",
        "plt.hist(n, bins=20, range=(0, 500))\n",
        "plt.title(\"Histogram\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWX0lEQVR4nO3dfbCedX3n8fenpKiIEh6OLE1iT5RUB5lWMYNh7XSttBDQGv6gDixdoptpZlbqWtcdC3ZXulJ2YLcjhbGyTSUruAwPS23JAhazgOO4y1MQBMJDc4RgkgI5kgRkqWjgu3/cv0NvjycP577PQ3LO+zVzz31d39/vuq7fLxzO51wP9zmpKiRJs9svTPcAJEnTzzCQJBkGkiTDQJKEYSBJwjCQJGEYaBZLsj7JB6Z7HNK+wDDQjJVkY5LfGlX7WJLvAFTVu6rqW3vYx2CSSjJnEocqTTvDQJpGhoz2FYaBZq3uM4ckxydZl+SFJM8m+WLr9u32viPJi0lOSPILSf5DkqeSbE1yVZJDuvZ7dmt7Lsl/HHWcP0lyQ5L/keQF4GPt2Hcm2ZHk6SRfSnJg1/4qySeSbEjyoyQXJHl7kv/bxnt9d3+pF4aB1HEpcGlVvRl4O3B9q/9Ge59bVQdX1Z3Ax9rrN4G3AQcDXwJIcgzwZeAs4CjgEGDeqGMtA24A5gJXA68AnwaOAE4ATgQ+MWqbk4H3AkuAzwKrgN8DFgDHAmf2MXfJMNCM97ftJ+4dSXbQ+UY9lp8CRyc5oqperKq7drPPs4AvVtUTVfUicB5wRrvkczrwv6rqO1X1E+DzwOhfAHZnVf1tVb1aVf9YVfdV1V1VtbOqNgJ/CfyLUdv8l6p6oarWAw8D32zHfx74BvCevf8nkX6eYaCZ7rSqmjvy4ud/4h6xAvgV4LEk9yb58G72+UvAU13rTwFzgCNb26aRhqp6CXhu1PabuleS/EqSm5I80y4d/Wc6Zwndnu1a/scx1g/ezXilPTIMJKCqNlTVmcBbgIuBG5K8kZ//qR7gH4Bf7lp/K7CTzjfop4H5Iw1J3gAcPvpwo9YvBx4DFrXLVJ8D0vtspPEzDCQgye8lGaiqV4EdrfwqMNze39bV/Rrg00kWJjmYzk/y11XVTjr3An4nyT9vN3X/hD1/Y38T8ALwYpJ3Av9mouYl7S3DQOpYCqxP8iKdm8lntOv5LwEXAv+n3XdYAqwGvkbnSaMngR8DnwRo1/Q/CVxL5yzhRWAr8PJujv3vgX8J/Aj4K+C6iZ+etHvxj9tIk6edOeygcwnoyekej7QrnhlIEyzJ7yQ5qN1z+DPgIWDj9I5K2j3DQJp4y+jcZP4HYBGdS06egmuf5mUiSZJnBpKkzgdl9ktHHHFEDQ4OTvcwJGm/ct999/2wqgZG1/fbMBgcHGTdunXTPQxJ2q8keWqsupeJJEmGgSTJMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJLEfvwJ5H4Mnntzz9tuvOhDEzgSSdo3eGYgSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiT2IgySrE6yNcnDY7R9JkklOaKtJ8llSYaSPJjkuK6+y5NsaK/lXfX3JnmobXNZkkzU5CRJe2dvzgy+CiwdXUyyADgJ+EFX+RRgUXutBC5vfQ8DzgfeBxwPnJ/k0LbN5cDvd233c8eSJE2uPYZBVX0b2DZG0yXAZ4Hqqi0DrqqOu4C5SY4CTgbWVtW2qtoOrAWWtrY3V9VdVVXAVcBp/U1JkjRePd0zSLIM2FJV3xvVNA/Y1LW+udV2V988Rn1Xx12ZZF2SdcPDw70MXZI0hnGHQZKDgM8Bn5/44exeVa2qqsVVtXhgYGCqDy9JM1YvZwZvBxYC30uyEZgPfDfJPwO2AAu6+s5vtd3V549RlyRNoXGHQVU9VFVvqarBqhqkc2nnuKp6BlgDnN2eKloCPF9VTwO3AiclObTdOD4JuLW1vZBkSXuK6GzgxgmamyRpL+3No6XXAHcC70iyOcmK3XS/BXgCGAL+CvgEQFVtAy4A7m2vL7Qarc9X2jbfB77R21QkSb3a4186q6oz99A+2LVcwDm76LcaWD1GfR1w7J7GIUmaPH4CWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiT27m8gr06yNcnDXbX/muSxJA8m+Zskc7vazksylOTxJCd31Ze22lCSc7vqC5Pc3erXJTlwIicoSdqzvTkz+CqwdFRtLXBsVf0q8PfAeQBJjgHOAN7VtvlykgOSHAD8BXAKcAxwZusLcDFwSVUdDWwHVvQ1I0nSuO0xDKrq28C2UbVvVtXOtnoXML8tLwOuraqXq+pJYAg4vr2GquqJqvoJcC2wLEmADwI3tO2vBE7rc06SpHGaiHsG/xr4RlueB2zqatvcaruqHw7s6AqWkbokaQr1FQZJ/hjYCVw9McPZ4/FWJlmXZN3w8PBUHFKSZoWewyDJx4APA2dVVbXyFmBBV7f5rbar+nPA3CRzRtXHVFWrqmpxVS0eGBjodeiSpFF6CoMkS4HPAh+pqpe6mtYAZyR5XZKFwCLgHuBeYFF7cuhAOjeZ17QQuQM4vW2/HLixt6lIknq1N4+WXgPcCbwjyeYkK4AvAW8C1iZ5IMl/A6iq9cD1wCPA3wHnVNUr7Z7AHwC3Ao8C17e+AH8E/LskQ3TuIVwxoTOUJO3RnD11qKozxyjv8ht2VV0IXDhG/RbgljHqT9B52kiSNE38BLIkyTCQJBkGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJLEXvwKa/2swXNv7nnbjRd9aAJHIkkTxzMDSZJhIEkyDCRJGAaSJPYiDJKsTrI1ycNdtcOSrE2yob0f2upJclmSoSQPJjmua5vlrf+GJMu76u9N8lDb5rIkmehJSpJ2b2/ODL4KLB1VOxe4raoWAbe1dYBTgEXttRK4HDrhAZwPvA84Hjh/JEBan9/v2m70sSRJk2yPYVBV3wa2jSovA65sy1cCp3XVr6qOu4C5SY4CTgbWVtW2qtoOrAWWtrY3V9VdVVXAVV37kiRNkV7vGRxZVU+35WeAI9vyPGBTV7/Nrba7+uYx6mNKsjLJuiTrhoeHexy6JGm0vm8gt5/oawLGsjfHWlVVi6tq8cDAwFQcUpJmhV7D4Nl2iYf2vrXVtwALuvrNb7Xd1eePUZckTaFew2ANMPJE0HLgxq762e2poiXA8+1y0q3ASUkObTeOTwJubW0vJFnSniI6u2tfkqQpssffTZTkGuADwBFJNtN5Kugi4PokK4CngI+27rcApwJDwEvAxwGqaluSC4B7W78vVNXITelP0Hli6Q3AN9pLkjSF9hgGVXXmLppOHKNvAefsYj+rgdVj1NcBx+5pHJKkyeMnkCVJhoEkyTCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAk0WcYJPl0kvVJHk5yTZLXJ1mY5O4kQ0muS3Jg6/u6tj7U2ge79nNeqz+e5OT+piRJGq+ewyDJPODfAour6ljgAOAM4GLgkqo6GtgOrGibrAC2t/olrR9JjmnbvQtYCnw5yQG9jkuSNH79XiaaA7whyRzgIOBp4IPADa39SuC0trysrdPaT0ySVr+2ql6uqieBIeD4PsclSRqHnsOgqrYAfwb8gE4IPA/cB+yoqp2t22ZgXlueB2xq2+5s/Q/vro+xzc9IsjLJuiTrhoeHex26JGmUfi4THUrnp/qFwC8Bb6RzmWfSVNWqqlpcVYsHBgYm81CSNKv0c5not4Anq2q4qn4KfB14PzC3XTYCmA9sactbgAUArf0Q4Lnu+hjbSJKmQD9h8ANgSZKD2rX/E4FHgDuA01uf5cCNbXlNW6e1315V1epntKeNFgKLgHv6GJckaZzm7LnL2Krq7iQ3AN8FdgL3A6uAm4Frk/xpq13RNrkC+FqSIWAbnSeIqKr1Sa6nEyQ7gXOq6pVexyVJGr+ewwCgqs4Hzh9VfoIxngaqqh8Dv7uL/VwIXNjPWCRJvfMTyJIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiS6DMMksxNckOSx5I8muSEJIclWZtkQ3s/tPVNksuSDCV5MMlxXftZ3vpvSLK830lJksan3zODS4G/q6p3Ar8GPAqcC9xWVYuA29o6wCnAovZaCVwOkOQw4HzgfcDxwPkjASJJmho9h0GSQ4DfAK4AqKqfVNUOYBlwZet2JXBaW14GXFUddwFzkxwFnAysraptVbUdWAss7XVckqTx6+fMYCEwDPz3JPcn+UqSNwJHVtXTrc8zwJFteR6wqWv7za22q/rPSbIyybok64aHh/sYuiSpWz9hMAc4Dri8qt4D/D/+6ZIQAFVVQPVxjJ9RVauqanFVLR4YGJio3UrSrNdPGGwGNlfV3W39Bjrh8Gy7/EN739ratwALuraf32q7qkuSpkjPYVBVzwCbkryjlU4EHgHWACNPBC0HbmzLa4Cz21NFS4Dn2+WkW4GTkhzabhyf1GqSpCkyp8/tPwlcneRA4Ang43QC5vokK4CngI+2vrcApwJDwEutL1W1LckFwL2t3xeqaluf49onDZ57c1/bb7zoQxM0Ekn6WX2FQVU9ACweo+nEMfoWcM4u9rMaWN3PWCRJvfMTyJIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCQxAWGQ5IAk9ye5qa0vTHJ3kqEk17W/j0yS17X1odY+2LWP81r98SQn9zsmSdL4TMSZwaeAR7vWLwYuqaqjge3AilZfAWxv9UtaP5IcA5wBvAtYCnw5yQETMC5J0l7qKwySzAc+BHylrQf4IHBD63IlcFpbXtbWae0ntv7LgGur6uWqehIYAo7vZ1ySpPHp98zgz4HPAq+29cOBHVW1s61vBua15XnAJoDW/nzr/1p9jG1+RpKVSdYlWTc8PNzn0CVJI3oOgyQfBrZW1X0TOJ7dqqpVVbW4qhYPDAxM1WElacab08e27wc+kuRU4PXAm4FLgblJ5rSf/ucDW1r/LcACYHOSOcAhwHNd9RHd20iSpkDPZwZVdV5Vza+qQTo3gG+vqrOAO4DTW7flwI1teU1bp7XfXlXV6me0p40WAouAe3odlyRp/Po5M9iVPwKuTfKnwP3AFa1+BfC1JEPANjoBQlWtT3I98AiwEzinql6ZhHFJknZhQsKgqr4FfKstP8EYTwNV1Y+B393F9hcCF07EWCRJ4+cnkCVJhoEkyTCQJGEYSJIwDCRJTM6jpZokg+fe3PO2Gy/60ASORNJM45mBJMkwkCQZBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJPsIgyYIkdyR5JMn6JJ9q9cOSrE2yob0f2upJclmSoSQPJjmua1/LW/8NSZb3Py1J0nj0c2awE/hMVR0DLAHOSXIMcC5wW1UtAm5r6wCnAIvaayVwOXTCAzgfeB9wPHD+SIBIkqZGz2FQVU9X1Xfb8o+AR4F5wDLgytbtSuC0trwMuKo67gLmJjkKOBlYW1Xbqmo7sBZY2uu4JEnjNyF/3CbJIPAe4G7gyKp6ujU9AxzZlucBm7o229xqu6qPdZyVdM4qeOtb3zoRQ581/MM4knan7xvISQ4G/hr4w6p6obutqgqofo/Rtb9VVbW4qhYPDAxM1G4ladbrKwyS/CKdILi6qr7eys+2yz+0962tvgVY0LX5/FbbVV2SNEX6eZoowBXAo1X1xa6mNcDIE0HLgRu76me3p4qWAM+3y0m3AiclObTdOD6p1SRJU6SfewbvB/4V8FCSB1rtc8BFwPVJVgBPAR9tbbcApwJDwEvAxwGqaluSC4B7W78vVNW2PsYlSRqnnsOgqr4DZBfNJ47Rv4BzdrGv1cDqXsciSeqPn0CWJE3Mo6Wa2XwsVZr5PDOQJBkGkiTDQJKEYSBJwjCQJGEYSJLw0VJNMh9LlfYPnhlIkgwDSZJhIEnCewbah/VzvwG85yCNh2cGkiTPDDRz+SSTtPc8M5AkeWYgjcWzCs02hoE0wQwS7Y+8TCRJ2nfODJIsBS4FDgC+UlUXTfOQpCnX7+O0/fCsZHbbJ8IgyQHAXwC/DWwG7k2ypqoemd6RSbPHdAbRdDEA/8k+EQbA8cBQVT0BkORaYBlgGEiaNPtjAE5WgO0rYTAP2NS1vhl43+hOSVYCK9vqi0ke7/F4RwA/7HHb/ZVznh1m25xn23zJxX3P+ZfHKu4rYbBXqmoVsKrf/SRZV1WLJ2BI+w3nPDvMtjnPtvnC5M15X3maaAuwoGt9fqtJkqbAvhIG9wKLkixMciBwBrBmmsckSbPGPnGZqKp2JvkD4FY6j5aurqr1k3jIvi817Yec8+ww2+Y82+YLkzTnVNVk7FeStB/ZVy4TSZKmkWEgSZpdYZBkaZLHkwwlOXe6xzNRkqxOsjXJw121w5KsTbKhvR/a6klyWfs3eDDJcdM38t4lWZDkjiSPJFmf5FOtPmPnneT1Se5J8r025//U6guT3N3mdl17CIMkr2vrQ619cDrH36skByS5P8lNbX1GzxcgycYkDyV5IMm6VpvUr+1ZEwZdv/LiFOAY4Mwkx0zvqCbMV4Glo2rnArdV1SLgtrYOnfkvaq+VwOVTNMaJthP4TFUdAywBzmn/PWfyvF8GPlhVvwa8G1iaZAlwMXBJVR0NbAdWtP4rgO2tfknrtz/6FPBo1/pMn++I36yqd3d9pmByv7arala8gBOAW7vWzwPOm+5xTeD8BoGHu9YfB45qy0cBj7flvwTOHKvf/vwCbqTzu61mxbyBg4Dv0vmk/g+BOa3+2tc5nafzTmjLc1q/TPfYxznP+e0b3weBm4DM5Pl2zXsjcMSo2qR+bc+aMwPG/pUX86ZpLFPhyKp6ui0/AxzZlmfcv0O7HPAe4G5m+LzbJZMHgK3AWuD7wI6q2tm6dM/rtTm39ueBw6d2xH37c+CzwKtt/XBm9nxHFPDNJPe1X8MDk/y1vU98zkCTq6oqyYx8hjjJwcBfA39YVS8kea1tJs67ql4B3p1kLvA3wDuneUiTJsmHga1VdV+SD0z3eKbYr1fVliRvAdYmeay7cTK+tmfTmcFs+5UXzyY5CqC9b231GfPvkOQX6QTB1VX19Vae8fMGqKodwB10LpPMTTLyg133vF6bc2s/BHhuiofaj/cDH0myEbiWzqWiS5m5831NVW1p71vphP7xTPLX9mwKg9n2Ky/WAMvb8nI619RH6me3JxCWAM93nXruN9I5BbgCeLSqvtjVNGPnnWSgnRGQ5A107pE8SicUTm/dRs955N/idOD2aheV9wdVdV5Vza+qQTr/v95eVWcxQ+c7Iskbk7xpZBk4CXiYyf7anu4bJVN8U+ZU4O/pXGf94+kezwTO6xrgaeCndK4XrqBzrfQ2YAPwv4HDWt/Qearq+8BDwOLpHn+Pc/51OtdVHwQeaK9TZ/K8gV8F7m9zfhj4fKu/DbgHGAL+J/C6Vn99Wx9q7W+b7jn0MfcPADfNhvm2+X2vvdaPfK+a7K9tfx2FJGlWXSaSJO2CYSBJMgwkSYaBJAnDQJKEYSBJwjCQJAH/H/tymN2FWQaKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qs3MAd1szWS"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pluu00U1PXCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a69e790-0a4c-4326-8380-0e92691567cb"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize  \n",
        "\n",
        "\n",
        "def train_and_evaluate_model(embeddings_dict, X_test_local=[], y_test_local=[], local=False, replace_oov=False):\n",
        "    url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\n",
        "    train = pd.read_csv(url)\n",
        "    X = np.array(train['comment_text'])\n",
        "    y = np.array(train['toxic_label'])\n",
        "    # X_test_local = []\n",
        "    # y_test_local = []\n",
        "    batch_size = 64\n",
        "\n",
        "    if local:\n",
        "        url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/local_data_301.csv\"\n",
        "        test_local = pd.read_csv(url)\n",
        "        X_test_local = np.array(test_local['Comment'])\n",
        "        y_test_local = np.array(test_local['Insult'])\n",
        "        batch_size = 16\n",
        "\n",
        "    # reduce size for faster training\n",
        "    size = X.shape[0]\n",
        "    # size = 10000\n",
        "\n",
        "    X = X[:size]\n",
        "    y = y[:size]\n",
        "\n",
        "    train_sentences, val_sentences, test_sentences, y_train, y_val, y_test, vocab, idx2word \\\n",
        "    = preprocess_input(X, y, seq_len=200, X_test_local=X_test_local, y_test_local=y_test_local, \\\n",
        "                       embeddings=embeddings_dict, replace_oov=replace_oov)\n",
        "\n",
        "    train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(y_train))\n",
        "    val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(y_val))\n",
        "    test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(y_test))\n",
        "\n",
        "    \n",
        "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "    val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "    output_size = 1\n",
        "    embedding_dim = len(list(embeddings_dict.values())[0])\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    # print(device)\n",
        "\n",
        "    model = SentimentNet(vocab, embeddings_dict, output_size, embedding_dim, hidden_dim=128, n_layers=2, drop_prob=0.2)\n",
        "    model.to(device)\n",
        "\n",
        "    lr=0.0003\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n",
        "\n",
        "    epochs = 5\n",
        "    counter = 0\n",
        "    print_every = int(0.5 * len(y_train) / batch_size)\n",
        "    clip = 5\n",
        "    valid_loss_min = np.Inf\n",
        "    best_model_so_far = None\n",
        "\n",
        "    # Set seed\n",
        "    torch.manual_seed(1)\n",
        "    np.random.seed(0)\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "        h = model.init_hidden(batch_size)\n",
        "        \n",
        "        for inputs, labels in train_loader:\n",
        "            counter += 1\n",
        "            h = tuple([e.data for e in h])\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            model.zero_grad()\n",
        "            output, h = model(inputs, h)\n",
        "            loss = criterion(output.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "            # lr_scheduler.step()\n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "                val_h = model.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                model.eval()\n",
        "                for val_input, val_label in val_loader:\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    val_input, val_label = val_input.to(device), val_label.to(device)\n",
        "                    out, val_h = model(val_input, val_h)\n",
        "                    val_loss = criterion(out.squeeze(), val_label.float())\n",
        "                    val_losses.append(val_loss.item())\n",
        "                    \n",
        "                model.train()\n",
        "                print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "                if np.mean(val_losses) <= valid_loss_min:\n",
        "                    # torch.save(model.state_dict(), './state_dict.pt')\n",
        "                    best_model_so_far = copy.deepcopy(model)\n",
        "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                    valid_loss_min = np.mean(val_losses)\n",
        "\n",
        "    test_losses = []\n",
        "    h = model.init_hidden(batch_size)\n",
        "    test_inputs = np.array([])\n",
        "    outputs = np.array([])\n",
        "    test_labels = np.array([])\n",
        "\n",
        "    # load the best model which was saved previously\n",
        "    model =  best_model_so_far\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            h = tuple([each.data for each in h])\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            output, h = model(inputs, h)\n",
        "            test_loss = criterion(output.squeeze(), labels.float())\n",
        "            test_losses.append(test_loss.item())\n",
        "            pred = torch.round(output.squeeze())\n",
        "\n",
        "            if len(test_inputs) == 0:\n",
        "                test_inputs = inputs.cpu().numpy()\n",
        "            else:\n",
        "                test_inputs = np.concatenate([test_inputs, inputs.cpu().numpy()])\n",
        "            outputs = np.concatenate([outputs, pred.cpu().numpy()])\n",
        "            test_labels = np.concatenate([test_labels, labels.cpu().numpy()])\n",
        "        \n",
        "        f1score = f1_score(np.array(test_labels), outputs)\n",
        "        recall = recall_score(np.array(test_labels), outputs)\n",
        "        precision = precision_score(np.array(test_labels), outputs)\n",
        "        accuracy = accuracy_score(np.array(test_labels), outputs)\n",
        "        print(\"##############################################################\")\n",
        "        print(\"Test Statistics:\")\n",
        "        print(f\"Test F1 score: {f1score}\")\n",
        "        print(f\"Test recall: {recall}\")\n",
        "        print(f\"Test precision: {precision}\")\n",
        "        print(f\"Test accuracy: {accuracy}\")\n",
        "        print(\"##############################################################\")\n",
        "        # Print some wrong predictions\n",
        "        for i, input in enumerate(test_inputs[:100]):\n",
        "            if outputs[i] != test_labels[i]:\n",
        "              input_no_pad = input[input != 0]\n",
        "              print(f\"Label: {test_labels[i]}, Prediction: {outputs[i]}\")\n",
        "              print(\" \".join([idx2word[i] for i in input_no_pad]))\n",
        "              print()\n",
        "        return f1score, recall, precision, accuracy\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Wpx-5n3wRj"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3NrmQJuCbcy",
        "outputId": "7398b791-6d21-4199-9748-5a3cf7e79357"
      },
      "source": [
        "train_and_evaluate_model(embeddings_dict=twitter_embeddings_dict, local=False, replace_oov=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------Training input statistics----------------\n",
            "Preview of first 50 OOV words in test data:  ['termnot', 'один', 'Mallow', 'central-Europe', 'copy-right', 'left-facing', 'soforth', 'Canaan', 'Timmy', 'intuitive.LexVacPac3', 'slagging', 'DD214', 's=143', 'Endoscope5', 'emulsions', '6ft', 'nouvelle', 'bikini', 'Ivanov', 'pedophilic', 'rebus', 'Pershing', 'alledy', 'Hypocracy', 'appended', '//www.questia.com/library/encyclopedia/columbus_christopher.jsp', 'NON-FACT', 'Linden', 'Grazie', 'terwible', 'grandpa', 'Sedunary', 'well-supported', 'Felician', 'contras', 'Lithuanians', 'bull-shit', 'LOVCKED', 'pluralized', '℥·start', 'GEniusboyIII', '==An', 'image-forging', '//www.parliament.nsw.gov.au/prod/lc/lctabdoc.nsf/cccc870c6126b1b6ca2571ee000318a4/6e2b81f5c474172fca257cee0023c16a', 'Clemmensen', 'Dunes.jpg', '86.178.140.114', 'Account==', 'checker', '978-966-671-179-6']\n",
            "Total number of OOV words in test data:  9537\n",
            "Max tokens in training data:  4948\n",
            "Average tokens:  75.06390318054257\n",
            "----------------------------------------------------------\n",
            "Epoch: 1/5... Step: 267... Loss: 0.206243... Val Loss: 0.300660\n",
            "Validation loss decreased (inf --> 0.300660).  Saving model ...\n",
            "Epoch: 1/5... Step: 534... Loss: 0.282942... Val Loss: 0.230733\n",
            "Validation loss decreased (0.300660 --> 0.230733).  Saving model ...\n",
            "Epoch: 2/5... Step: 801... Loss: 0.311808... Val Loss: 0.224521\n",
            "Validation loss decreased (0.230733 --> 0.224521).  Saving model ...\n",
            "Epoch: 2/5... Step: 1068... Loss: 0.141525... Val Loss: 0.221497\n",
            "Validation loss decreased (0.224521 --> 0.221497).  Saving model ...\n",
            "Epoch: 3/5... Step: 1335... Loss: 0.082417... Val Loss: 0.191997\n",
            "Validation loss decreased (0.221497 --> 0.191997).  Saving model ...\n",
            "Epoch: 3/5... Step: 1602... Loss: 0.095517... Val Loss: 0.211743\n",
            "Epoch: 4/5... Step: 1869... Loss: 0.129379... Val Loss: 0.192796\n",
            "Epoch: 4/5... Step: 2136... Loss: 0.232624... Val Loss: 0.189595\n",
            "Validation loss decreased (0.191997 --> 0.189595).  Saving model ...\n",
            "Epoch: 5/5... Step: 2403... Loss: 0.054560... Val Loss: 0.228943\n",
            "Epoch: 5/5... Step: 2670... Loss: 0.185032... Val Loss: 0.185128\n",
            "Validation loss decreased (0.189595 --> 0.185128).  Saving model ...\n",
            "##############################################################\n",
            "Test Statistics:\n",
            "Test F1 score: 0.9048665620094192\n",
            "Test recall: 0.9149206349206349\n",
            "Test precision: 0.8950310559006212\n",
            "Test accuracy: 0.9282670454545454\n",
            "##############################################################\n",
            "Label: 0.0, Prediction: 1.0\n",
            "`` threats the comment was deleted for the reason i said . i did n't want distraction on the mediation . so lets do this here . i think you think you are being much _UNKNOWN than you are . stay within wikipedias rules and the ferocious hatred i have for you will not find outside expression . keep provoking me on this forum and it will . i did n't come into your life and do damage , you came into mine . you seem to feel perfectly free to attack me , degrade me and _UNKNOWN me . you have no remorse no conscience . there is a constant _UNKNOWN of _UNKNOWN and dishonesty it just about everything you write . if you were genuinely _UNKNOWN you would be polite . you are n't _UNKNOWN this is just one more stupid side show trick of yours . and if you do genuinely feel a little _UNKNOWN , good , you should . you have crossed way over the line . keep your dialogue to a minimum and keep it on topic . i do n't want to hear about your assessment of me on various forums . you\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` also , _UNKNOWN , name calling people you disagree with `` '' _UNKNOWN '' '' really _UNKNOWN wikipedia and perpetuates the impression that most administrators are bigots . ''\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "table change it back to the list , is is so stupid and takes longer to change and edit stuff here my vote is to change it back . agrees : 1 disagrees : 0\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i guess it 's easy to dish out criticism but you ca n't take it . do n't blame me for the fact that coldplay is a collection of _UNKNOWN music _UNKNOWN , _UNKNOWN by a badly diseased western media . i know what makes for music quality and coldplay is certainly anything but .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "( utc ) one of them reminded me of paris hilton _UNKNOWN iq _UNKNOWN insult to the latter , i 'm still wondering if it was her _UNKNOWN to play the dumb blond . the other reminds me of madonna . both have big _UNKNOWN that ai n't particularly sexist as it 's referred to in the show . as for huffington , she 's a very intelligent woman who got a very handsome divorce settlement , before leaving the _UNKNOWN _UNKNOWN , 4 june 2015\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i 'm going to be honest with you , i 'm not going to pass your articles as ga , because frankly , they suck . but that does n't mean an agreement ca n't be reached upon where your articles could still potentially reach ga. first thing you need to know , is that good article status is a crock of shit . the criteria for it is a fair model , and it is not the problem . the problem is that it is not governed by a committee . any bozo , like yourself , is allowed to review an article and ultimately decide if the article , in their mind , merits inclusion into _UNKNOWN ( club green dot ) . the bar sits firmly in the mud . snakes _UNKNOWN across it . my articles , on the other hand , never have enjoyed the benefits of exception . with extreme prejudice , they have been held to the highest possible standard set by the criteria . now we get to your ga review of _UNKNOWN bells , which i would like a full explanation for . that 's one of the agreements that i\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "stupid bots deleting pics i would like to ask a human to review the entry , and possibly add photos from the links i have provided . i can not figure out how to do it myself . also , someone should either _UNKNOWN or shut down this bot . april 2008 welcome to wikipedia . although everyone is welcome to contribute constructively to the encyclopedia , one or more of the external links you added do not comply with our guidelines for external links and have been removed . wikipedia is not a collection of links ; nor should it be used for advertising or promotion . since wikipedia uses nofollow tags , external links do not alter search engine rankings . if you feel the link should be added to the article , please discuss it on the before reinserting it . please take a look at the welcome page to learn more about contributing to this encyclopedia . your edit here was reverted by an automated bot that attempts to remove unwanted links and spam from wikipedia . if you were trying to insert a good link , please accept my creator 's apologies , but note\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "enjoy your day of fun ... ha . i 'm still cleaning up . - talk\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` hello please remove _UNKNOWN language from your edit here before you want someone to discuss that with you . thanks . - wiki me _UNKNOWN ok ! great ! which part exactly is `` '' inappropriate '' '' and by whose standards exactly ? thanks . let me know , so that i can do whatever it is that you command / demand of me . and the words that you want removed , please let me know what i can / should / am allowed to replace them with ? thanks for your help ! and your concern ! much appreciated ! thanks ! ( ) ''\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9048665620094192,\n",
              " 0.9149206349206349,\n",
              " 0.8950310559006212,\n",
              " 0.9282670454545454)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3dDlYa95mbT"
      },
      "source": [
        "# Evaluate Model on Local Data (Using training set vocab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb9OrUKM3UiZ",
        "outputId": "2a609bfc-fd28-401e-963f-fc45cf89cb70"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/local_data_301.csv\"\n",
        "test_local = pd.read_csv(url)\n",
        "X_test_local = np.array(test_local['Comment'])\n",
        "y_test_local = np.array(test_local['Insult'])\n",
        "# train_and_evaluate_model(embeddings_dict=twitter_embeddings_dict, X_test_local=X_test_local, y_test_local=y_test_local)\n",
        "train_and_evaluate_model(embeddings_dict=twitter_embeddings_dict, local=True, replace_oov=False)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------Training input statistics----------------\n",
            "Preview of first 50 OOV words in test data:  ['hag', 'tiong', 'hardworking', 'suay', 'uncooperative', 'fw', 'accomodation', 'pisai', 'Aiyo', 'uplorry', 'Singaporean', 'xia', 'mah..', '🇸🇬', 'CECA', 'knn', 'soups', 'Diam', 'attractive/can', 'sda', 'woof', 'liao', 'hardship', 'edmwers', 'shouldn', 'laosai', 'sinkie', 'unwashed', 'slipper', 'wuhan', 'smokers', 'pariah', 'satki', 'hantu', 'lim', \"'strawberry\", '9gag', 'leeways', 'DIU~~~~', 'tummy', 'hotspots', '100/day', 'buay', \"'snowflakes\", 'Groovelife', 'bodyguards', 'cb', 'COVID19', 'Frontline', '10million']\n",
            "Total number of OOV words in test data:  168\n",
            "Max tokens in training data:  4948\n",
            "Average tokens:  75.06390318054257\n",
            "----------------------------------------------------------\n",
            "Epoch: 1/5... Step: 1069... Loss: 0.319521... Val Loss: 0.672348\n",
            "Validation loss decreased (inf --> 0.672348).  Saving model ...\n",
            "Epoch: 1/5... Step: 2138... Loss: 0.166250... Val Loss: 0.585584\n",
            "Validation loss decreased (0.672348 --> 0.585584).  Saving model ...\n",
            "Epoch: 2/5... Step: 3207... Loss: 0.154387... Val Loss: 0.648006\n",
            "Epoch: 2/5... Step: 4276... Loss: 0.033130... Val Loss: 0.705376\n",
            "Epoch: 3/5... Step: 5345... Loss: 0.295746... Val Loss: 0.648300\n",
            "Epoch: 3/5... Step: 6414... Loss: 0.140902... Val Loss: 0.748514\n",
            "Epoch: 4/5... Step: 7483... Loss: 0.112506... Val Loss: 0.719703\n",
            "Epoch: 4/5... Step: 8552... Loss: 0.112688... Val Loss: 0.715500\n",
            "Epoch: 5/5... Step: 9621... Loss: 0.204565... Val Loss: 0.762218\n",
            "Epoch: 5/5... Step: 10690... Loss: 0.005498... Val Loss: 0.775863\n",
            "##############################################################\n",
            "Test Statistics:\n",
            "Test F1 score: 0.618705035971223\n",
            "Test recall: 0.5972222222222222\n",
            "Test precision: 0.6417910447761194\n",
            "Test accuracy: 0.6319444444444444\n",
            "##############################################################\n",
            "Label: 1.0, Prediction: 0.0\n",
            "maybe need to cane these metal teeth people then they will comply\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "_UNKNOWN it 's just diff cultures _UNKNOWN men are known to spoil their _UNKNOWN and spend money on them like water like that , , they are literally brought up that way lol , , , tbh if he is not spending _UNKNOWN on you despite his high salary , why not u just communicate what u want ? ? ? why u want to post here let ppl flame u ? ? ? im sure you and ur bf stable enough to communicate your expectations right ? if buying a _UNKNOWN bag for u makes u happy , i 'm sure he 'll go ahead and do it since he has the monetary means ? ? ? unless he is saving up for something then y'all can communicate about it like mature people ? ? ? there is no right or wrong la _UNKNOWN just communicate pls\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN lang using _UNKNOWN wifi typing nonsense again..\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "do us a favour take a long walk on a short pier bye you will not be missed\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you know what is inside the _UNKNOWN bag behind them ! ! not all but if you are _UNKNOWN 1 . _UNKNOWN zero maintenance in it . 2 . overnight spilled sauce or dry up _UNKNOWN . 3 . _UNKNOWN _UNKNOWN uniform and helmets . 4 on the way _UNKNOWN or old shoe if _UNKNOWN see one . 5 . last but not least all sort of things you can not imagine including _UNKNOWN tiny little ones . enjoy your meal .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "put him to sleep . no need to talk\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "u look like those _UNKNOWN ah _UNKNOWN\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you should ask your masters do n't act smart _UNKNOWN _UNKNOWN _UNKNOWN circuit _UNKNOWN end up so much confusion . want to lock down , call it a lockdown , then you do n't have to blame people for not cooperating .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you come here i pay you 600 u bend over so i can shove a broom up ur arse : x\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "behind closed doors say u so _UNKNOWN _UNKNOWN . come out say no _UNKNOWN will be left behind _UNKNOWN .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i am christian but the sex is so good .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "coach red pill mentioned that women look at men and divide them into general 80 % of the men ( who got no chance in dating them ) and the top 20 % of men ( in terms of wealth , looks and fame ) . good to see these top 20 % men fall .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN is the gold standard . too arrogant to be seen to accept help .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "if you wan na know why the older generation calls us _UNKNOWN ' or the _UNKNOWN generation ' , simply read this post\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN , erection still retain power . you guys really can not make it\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "_UNKNOWN here talk big to hide the filthy conditions that existed in _UNKNOWN .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "you need professional help .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "circuit _UNKNOWN really accelerate their mental illness\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "ts and white dogs if you all have no ideas and suggestions to help _UNKNOWN , ministers and the govt to do better . why do n't you shut the fark up ?\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "_UNKNOWN _UNKNOWN man come _UNKNOWN _UNKNOWN clap for you\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "ah ah deserve it for not hiring _UNKNOWN .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "because they always tend to go for the `` exotic looking '' ones ( read - ugly ) , with extremely small eyes or exaggerated asian features . most asian guys wo n't even find them attractive anyway !\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN rings ... silicone . cheap , comfortable , _UNKNOWN and most importantly , it won ’ t _UNKNOWN your finger\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "why you _UNKNOWN and vomit still drink milk tea ? try to refrain from dairy stuff . it creates nonsense with your _UNKNOWN acid .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "shameless _UNKNOWN . first , their _UNKNOWN has caused the foreign workers to be infected . now talk so big . foreign workers should have been provided adequate protection and facilities . why were they exposed to undue risk . _UNKNOWN are totally irresponsible .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "a big shame on the public institutions who had failed her . so much so that a victim has to go out on display in social media to get her story heard ! !\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "body slam the fking aggressors pls\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "since when he got drive to improve himself ? improve in what area ? at being useless ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "hence , may i appeal for all of you to stay in singapore , and avoid bringing back _UNKNOWN back into singapore upon your return ? this is for the sake of every _UNKNOWN , pr , and foreigner who responsibly chose to remain in singapore .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "life is unfair op . i 'm really sorry but this is how it is . if you are a scholar they own you\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "how you going to shut us up ? anyway , we are too good to take over govt . people like us who are honest , _UNKNOWN and skeptical are not suitable to be public servants .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "here a lot of those who complain richer than u , they are not loser haha\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN the pap . _UNKNOWN the wp . _UNKNOWN the _UNKNOWN . _UNKNOWN the _UNKNOWN . _UNKNOWN the psp . _UNKNOWN the ccp . _UNKNOWN the ussr . _UNKNOWN the usa . _UNKNOWN ts .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "boy boy , do you challenge someone who gives you sound advice ? _UNKNOWN that someone explained to you nicely but not being nasty . whether breaking law or not is another story . it is more about the personal moral and upbringing .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN _UNKNOWN , talk so much _UNKNOWN\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.618705035971223, 0.5972222222222222, 0.6417910447761194, 0.6319444444444444)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRXShM8RmY9t"
      },
      "source": [
        "# Evaluate Model on Local Data (Using Entire Embedding Vocab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd6CA8TNmZqI",
        "outputId": "bc2d71b2-cce9-44a7-e9b1-434b2a6caa48"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/local_data_301.csv\"\n",
        "test_local = pd.read_csv(url)\n",
        "X_test_local = np.array(test_local['Comment'])\n",
        "y_test_local = np.array(test_local['Insult'])\n",
        "# train_and_evaluate_model(embeddings_dict=twitter_embeddings_dict, X_test_local=X_test_local, y_test_local=y_test_local)\n",
        "train_and_evaluate_model(embeddings_dict=twitter_embeddings_dict, local=True, replace_oov=False)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding size:  1193514\n",
            "----------------Training input statistics----------------\n",
            "Substituted words:  {}\n",
            "Preview of first 50 OOV words in test data:  ['worse..', '3rd', 'sporeans', 'actually..', 'pg/mg', '1', 'brain.that', 'pisai', 'any1', '10', 'life..', 'whor*', 'ptui', 'uplorry', '🇸🇬', 'mah..', 'attractive/can', 'dumbccb', 'stuppiak', 'covid19', 'edmwers', 'peepur', 'laosai', 'sinkie', 'kbkb', 'satki', 'kumgongness', '.only', \"'strawberry\", 'leeways', '9gag', 'amitabha', 'non-prohibited', '.no', \"'fit\", '..dun', '.birkin', '100/day', '928', 'rah.but', '20', 'donch', \"'snowflakes\", '11th', '2k', '19', '3', 'siaolang', '10million', 'teachers.we']\n",
            "Total number of OOV words in test data:  131\n",
            "Max tokens in training data:  4948\n",
            "Average tokens:  75.06390318054257\n",
            "----------------------------------------------------------\n",
            "Epoch: 1/5... Step: 1069... Loss: 0.283302... Val Loss: 0.560283\n",
            "Validation loss decreased (inf --> 0.560283).  Saving model ...\n",
            "Epoch: 1/5... Step: 2138... Loss: 0.187160... Val Loss: 0.546186\n",
            "Validation loss decreased (0.560283 --> 0.546186).  Saving model ...\n",
            "Epoch: 2/5... Step: 3207... Loss: 0.105212... Val Loss: 0.640433\n",
            "Epoch: 2/5... Step: 4276... Loss: 0.024609... Val Loss: 0.697051\n",
            "Epoch: 3/5... Step: 5345... Loss: 0.300860... Val Loss: 0.660951\n",
            "Epoch: 3/5... Step: 6414... Loss: 0.235691... Val Loss: 0.689138\n",
            "Epoch: 4/5... Step: 7483... Loss: 0.104522... Val Loss: 0.656768\n",
            "Epoch: 4/5... Step: 8552... Loss: 0.113356... Val Loss: 0.624516\n",
            "Epoch: 5/5... Step: 9621... Loss: 0.212068... Val Loss: 0.748552\n",
            "Epoch: 5/5... Step: 10690... Loss: 0.013775... Val Loss: 0.721201\n",
            "##############################################################\n",
            "Test Statistics:\n",
            "Test F1 score: 0.6338028169014085\n",
            "Test recall: 0.625\n",
            "Test precision: 0.6428571428571429\n",
            "Test accuracy: 0.6388888888888888\n",
            "##############################################################\n",
            "Label: 1.0, Prediction: 0.0\n",
            "maybe need to cane these metal teeth people then they will comply\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "aiyo it 's just diff cultures _UNKNOWN men are known to spoil their gfs and spend money on them like water like that , , they are literally brought up that way lol , , , tbh if he is not spending lavishly on you despite his high salary , why not u just communicate what u want ? ? ? why u want to post here let ppl flame u ? ? ? im sure you and ur bf stable enough to communicate your expectations right ? if buying a chanel bag for u makes u happy , i 'm sure he 'll go ahead and do it since he has the monetary means ? ? ? unless he is saving up for something then _UNKNOWN can communicate about it like mature people ? ? ? there is no right or wrong la aiyo just communicate pls\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "do us a favour take a long walk on a short pier bye you will not be missed\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you know what is inside the greeny bag behind them ! ! not all but if you are _UNKNOWN _UNKNOWN . unwashed zero maintenance in it . _UNKNOWN . overnight spilled sauce or dry up soups . _UNKNOWN . greeny unwashed uniform and helmets . _UNKNOWN on the way slipper or old shoe if _UNKNOWN see one . _UNKNOWN . last but not least all sort of things you can not imagine including tinny tiny little ones . enjoy your meal .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "put him to sleep . no need to talk\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "u look like those chao ah tiong\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you should ask your masters do n't act smart gey kiang smlj circuit breaker end up so much confusion . want to lock down , call it a lockdown , then you do n't have to blame people for not cooperating .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you come here i pay you _UNKNOWN u bend over so i can shove a broom up ur arse : x\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "behind closed doors say u so xia suay . come out say no singaporean will be left behind lolol .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i am christian but the sex is so good .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "coach red pill mentioned that women look at men and divide them into general _UNKNOWN % of the men ( who got no chance in dating them ) and the top _UNKNOWN % of men ( in terms of wealth , looks and fame ) . good to see these top _UNKNOWN % men fall .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN is the gold standard . too arrogant to be seen to accept help .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "if you wan na know why the older generation calls us _UNKNOWN ' or the _UNKNOWN generation ' , simply read this post\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN , erection still retain power . you guys really can not make it\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "singaporeans here talk big to hide the filthy conditions that existed in _UNKNOWN .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "you need professional help .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "circuit breaker really accelerate their mental illness\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "ts and white dogs if you all have no ideas and suggestions to help singaporeans , ministers and the govt to do better . why do n't you shut the fark up ?\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "jin _UNKNOWN man come lim peh clap for you\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "ah ah deserve it for not hiring _UNKNOWN .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "because they always tend to go for the `` exotic looking '' ones ( read - ugly ) , with extremely small eyes or exaggerated asian features . most asian guys wo n't even find them attractive anyway !\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN rings _UNKNOWN silicone . cheap , comfortable , durable and most importantly , it won ’ t _UNKNOWN your finger\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "why you _UNKNOWN and vomit still drink milk tea ? try to refrain from dairy stuff . it creates nonsense with your tummy acid .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "shameless singaporeans . first , their carelessness has caused the foreign workers to be infected . now talk so big . foreign workers should have been provided adequate protection and facilities . why were they exposed to undue risk . singaporeans are totally irresponsible .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "a big shame on the public institutions who had failed her . so much so that a victim has to go out on display in social media to get her story heard ! !\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "looks like the officer is having a hard time trying to educate a donkey . reserved one goody bag for the officer once everything is over give him the bag of goody .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "body slam the fking aggressors pls\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "since when he got drive to improve himself ? improve in what area ? at being useless ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "hence , may i appeal for all of you to stay in singapore , and avoid bringing back _UNKNOWN back into singapore upon your return ? this is for the sake of every singaporean , pr , and foreigner who responsibly chose to remain in singapore .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "life is unfair op . i 'm really sorry but this is how it is . if you are a scholar they own you\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "how you going to shut us up ? anyway , we are too good to take over govt . people like us who are honest , hardworking and skeptical are not suitable to be public servants .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "here a lot of those who complain richer than u , they are not loser haha\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "siao liao i think cb going extend _UNKNOWN going take no pay leave already\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "boy boy , do you challenge someone who gives you sound advice ? somemore that someone explained to you nicely but not being nasty . whether breaking law or not is another story . it is more about the personal moral and upbringing .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "diam lah , talk so much cog\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6338028169014085, 0.625, 0.6428571428571429, 0.6388888888888888)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl4eSX-M7fy4"
      },
      "source": [
        "# Evaluate Model on twitter + ntu embeddings (with OOV replacement using edit distance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ze0Rdjc5lQT",
        "outputId": "d9a88ad3-ae19-4c38-f473-69d2a4d9674a"
      },
      "source": [
        "# combined with NTU comments wit\n",
        "train_and_evaluate_model(embeddings_dict=new_embeddings_dict_2, local=True, replace_oov=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding size:  1195762\n",
            "no\n",
            "Processed OOV:  1\n",
            "birkin\n",
            "Processed OOV:  2\n",
            "pisai\n",
            "Processed OOV:  3\n",
            "oppies\n",
            "Processed OOV:  4\n",
            "jjww\n",
            "Processed OOV:  5\n",
            "amitabha\n",
            "Processed OOV:  6\n",
            "epithets\n",
            "Processed OOV:  7\n",
            "300500\n",
            "Processed OOV:  8\n",
            "rahbut\n",
            "Processed OOV:  9\n",
            "942\n",
            "Processed OOV:  10\n",
            "928\n",
            "Processed OOV:  11\n",
            "whor\n",
            "Processed OOV:  12\n",
            "154th\n",
            "Processed OOV:  13\n",
            "sure\n",
            "Processed OOV:  14\n",
            "gahment\n",
            "Processed OOV:  15\n",
            "siaolang\n",
            "Processed OOV:  16\n",
            "dumbccb\n",
            "Processed OOV:  17\n",
            "ashle\n",
            "Processed OOV:  18\n",
            "blardy\n",
            "Processed OOV:  19\n",
            "it\n",
            "Processed OOV:  20\n",
            "stuppiak\n",
            "Processed OOV:  21\n",
            "fumigation\n",
            "Processed OOV:  22\n",
            "braiiiinnnnnnn\n",
            "Processed OOV:  23\n",
            "americanised\n",
            "Processed OOV:  24\n",
            "teacherswe\n",
            "Processed OOV:  25\n",
            "sinkies\n",
            "Processed OOV:  26\n",
            "life\n",
            "Processed OOV:  27\n",
            "worse\n",
            "Processed OOV:  28\n",
            "donch\n",
            "Processed OOV:  29\n",
            "nonprohibited\n",
            "Processed OOV:  30\n",
            "mrstinkie95\n",
            "Processed OOV:  31\n",
            "11th\n",
            "Processed OOV:  32\n",
            "ing\n",
            "Processed OOV:  33\n",
            "kumgongness\n",
            "Processed OOV:  34\n",
            "tiongs\n",
            "Processed OOV:  35\n",
            "property\n",
            "Processed OOV:  36\n",
            "actually\n",
            "Processed OOV:  37\n",
            "bloodyfools\n",
            "Processed OOV:  38\n",
            "dun\n",
            "Processed OOV:  39\n",
            "bossangery\n",
            "Processed OOV:  40\n",
            "overevaluated\n",
            "Processed OOV:  41\n",
            "woof\n",
            "Processed OOV:  42\n",
            "brainthat\n",
            "Processed OOV:  43\n",
            "sister\n",
            "Processed OOV:  44\n",
            "deliverymen\n",
            "Processed OOV:  45\n",
            "unverifiable\n",
            "Processed OOV:  46\n",
            "only\n",
            "Processed OOV:  47\n",
            "hot\n",
            "Processed OOV:  48\n",
            "fit\n",
            "Processed OOV:  49\n",
            "gffuture\n",
            "Processed OOV:  50\n",
            "50s60s\n",
            "Processed OOV:  51\n",
            "idiotsselfish\n",
            "Processed OOV:  52\n",
            "groovelife\n",
            "Processed OOV:  53\n",
            "deglove\n",
            "Processed OOV:  54\n",
            "ahmah\n",
            "Processed OOV:  55\n",
            "kwailan\n",
            "Processed OOV:  56\n",
            "limpeh\n",
            "Processed OOV:  57\n",
            "sinkie\n",
            "Processed OOV:  58\n",
            "lachina\n",
            "Processed OOV:  59\n",
            "uplorry\n",
            "Processed OOV:  60\n",
            "covidiots\n",
            "Processed OOV:  61\n",
            "dormitories\n",
            "Processed OOV:  62\n",
            "100day\n",
            "Processed OOV:  63\n",
            "leg\n",
            "Processed OOV:  64\n",
            "expenses\n",
            "Processed OOV:  65\n",
            "pappies\n",
            "Processed OOV:  66\n",
            "pgmg\n",
            "Processed OOV:  67\n",
            "peepur\n",
            "Processed OOV:  68\n",
            "northing\n",
            "Processed OOV:  69\n",
            "extramiles\n",
            "Processed OOV:  70\n",
            "pedestral\n",
            "Processed OOV:  71\n",
            "laosai\n",
            "Processed OOV:  72\n",
            "kbkb\n",
            "Processed OOV:  73\n",
            "satki\n",
            "Processed OOV:  74\n",
            "rubbisheverywhere\n",
            "Processed OOV:  75\n",
            "mah\n",
            "Processed OOV:  76\n",
            "sporeans\n",
            "Processed OOV:  77\n",
            "ourselves\n",
            "Processed OOV:  78\n",
            "again\n",
            "Processed OOV:  79\n",
            "sht\n",
            "Processed OOV:  80\n",
            "enforcements\n",
            "Processed OOV:  81\n",
            "yayapapa\n",
            "Processed OOV:  82\n",
            "mask\n",
            "Processed OOV:  83\n",
            "10million\n",
            "Processed OOV:  84\n",
            "pangsai\n",
            "Processed OOV:  85\n",
            "snowflakes\n",
            "Processed OOV:  86\n",
            "strawberry\n",
            "Processed OOV:  87\n",
            "unlucky\n",
            "Processed OOV:  88\n",
            "grabie\n",
            "Processed OOV:  89\n",
            "diu\n",
            "Processed OOV:  90\n",
            "garhemen\n",
            "Processed OOV:  91\n",
            "smoker\n",
            "Processed OOV:  92\n",
            "china\n",
            "Processed OOV:  93\n",
            "grabbie\n",
            "Processed OOV:  94\n",
            "ptui\n",
            "Processed OOV:  95\n",
            "cutpaste\n",
            "Processed OOV:  96\n",
            "motherfcuker\n",
            "Processed OOV:  97\n",
            "9gag\n",
            "Processed OOV:  98\n",
            "\n",
            "Processed OOV:  99\n",
            "antichina\n",
            "Processed OOV:  100\n",
            "antichinese\n",
            "Processed OOV:  101\n",
            "leeways\n",
            "Processed OOV:  102\n",
            "edmwers\n",
            "Processed OOV:  103\n",
            "attractivecan\n",
            "Processed OOV:  104\n",
            "----------------Training input statistics----------------\n",
            "Substituted words:  {'.no': '2nd', '.birkin': 'bikin', 'pisai': 'bisa', 'oppies': 'pies', 'jjww': 'aww', 'amitabha': 'amitabh', 'epithets': 'epithet', '300-500': '3000', 'rah.but': 'rambut', '942': '24h', '928': '29', 'whor*': '/or', '154th': '75th', '..sure': 'sutd', 'gahment': 'ahmet', 'siaolang': 'sialan', 'dumbccb': 'dumbb', 'ash*le': 'she', 'blardy': 'lady', 'it..': '71', 'stuppiak': 'stuppid', 'fumigation': 'fumigación', 'braiiiinnnnnnn': 'hair.nnnnand', 'americanised': 'americanista', 'teachers.we': 'teachers', 'sinkies': 'singles', 'life..': 'if…', 'worse..': \"'worse\", 'donch': \"don'…\", 'non-prohibited': 'contributed', 'mrstinkie95': 'stinkiest', '11th': '11', '****ing': '2nd', 'kumgongness': 'omgoodness', 'tiongs': 'things', 'property..': 'proper', 'actually..': 'actual…', 'bloodyfools': 'bloodyhell', '..dun': 'u…', 'boss.angery': 'boulanger', 'over-evaluated': 'reevaluated', \"'woof\": 'hooc', 'brain.that': 'brangkat', 'sister..': 'sister~', 'deliverymen': 'deliveryman', 'unverifiable': 'verifiable', '.only': 'on:1', \"'hot\": 'hooc', \"'fit\": 'fin…', 'gf/future': 'future', '50s/60s': '30260', 'idiots..selfish': 'distinguish', 'groovelife': 'lovelife', 'deglove': 'glove', 'ahmah': 'ama', 'kwailan': 'kalan', 'limpeh': 'limbei', 'sinkie': 'single', 'la..china': 'china', 'uplorry': 'lorry', 'covidiots': 'comidita', 'dormitories': 'dormitorio', '100/day': '10am', 'leg..': 'la~', 'expenses…': 'penses', 'pappies': 'parties', 'pg/mg': 'omg', 'peepur': 'keeper', 'northing': 'northill', 'extramiles': 'examples', 'pedestral': 'pedestrian', 'laosai': 'lantai', 'kbkb': 'kekw', 'satki': 'hati', 'rubbish.everywhere': 'musiceverywhere', 'mah..': 'la~', 'sporeans': 'sgporean', 'ourselves..': 'ourselves', 'again..': 'again', 'sh*t': 'sutd', 'enforcements': 'enforcement', 'yayapapa': 'yayayaya', 'mask..': 'maskne', '10million': 'million', 'pangsai': 'pandai', \"'snowflakes\": 'snowflake', \"'strawberry\": 'strawberry', 'unlucky..': 'lucky', 'grabie': 'grande', 'diu~~~~': 's/u', 'garhemen': 'parlemen', 'smoker…': 'shower', 'china..': 'chinyan', 'grabbie..': 'grazie', 'ptui': 'ntu/', 'cut-paste': 'chapaste', 'motherfcuker': 'motherfucker', '9gag': '9a', '🇸🇬': '71', 'anti-china': 'anticipa', 'anti-chinese': 'nnchinese', 'leeways': 'deejays', 'edmwers': 'embers', 'attractive/can': 'attractiven'}\n",
            "Preview of first 50 OOV words in test data:  ['sister..', 'edmwers', 'unlucky..', 'mrstinkie95', 'covidiots', 'mask..', 'ahmah', 'siaolang', '942', '****ing', 'garhemen', 'motherfcuker', 'grabbie..', 'non-prohibited', \"'woof\", 'sporeans', 'unverifiable', '..sure', \"'fit\", 'laosai', 'smoker…', 'idiots..selfish', '.birkin', 'kbkb', 'leg..', 'peepur', 'again..', '50s/60s', 'china..', '🇸🇬', '.only', 'limpeh', 'la..china', 'over-evaluated', 'whor*', '11th', 'it..', 'stuppiak', \"'snowflakes\", 'deglove', 'worse..', 'extramiles', 'satki', '928', 'boss.angery', 'sinkie', 'brain.that', 'attractive/can', 'yayapapa', 'gf/future']\n",
            "Total number of OOV words in test data:  104\n",
            "Max tokens in training data:  2086\n",
            "Average tokens:  81.15825\n",
            "----------------------------------------------------------\n",
            "Epoch: 1/5... Step: 250... Loss: 0.019566... Val Loss: 0.771300\n",
            "Validation loss decreased (inf --> 0.771300).  Saving model ...\n",
            "Epoch: 1/5... Step: 500... Loss: 0.317252... Val Loss: 0.827490\n",
            "Epoch: 2/5... Step: 750... Loss: 0.137900... Val Loss: 0.621995\n",
            "Validation loss decreased (0.771300 --> 0.621995).  Saving model ...\n",
            "Epoch: 2/5... Step: 1000... Loss: 0.066429... Val Loss: 0.855830\n",
            "Epoch: 3/5... Step: 1250... Loss: 0.065867... Val Loss: 0.796481\n",
            "Epoch: 3/5... Step: 1500... Loss: 0.088514... Val Loss: 0.586913\n",
            "Validation loss decreased (0.621995 --> 0.586913).  Saving model ...\n",
            "Epoch: 4/5... Step: 1750... Loss: 0.010340... Val Loss: 0.741584\n",
            "Epoch: 4/5... Step: 2000... Loss: 0.132197... Val Loss: 0.684298\n",
            "Epoch: 5/5... Step: 2250... Loss: 0.006488... Val Loss: 0.905750\n",
            "Epoch: 5/5... Step: 2500... Loss: 0.047895... Val Loss: 1.032867\n",
            "##############################################################\n",
            "Test Statistics:\n",
            "Test F1 score: 0.5045045045045045\n",
            "Test recall: 0.4\n",
            "Test precision: 0.6829268292682927\n",
            "Test accuracy: 0.6180555555555556\n",
            "##############################################################\n",
            "Label: 0.0, Prediction: 1.0\n",
            "do n't friend her anymore lor .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "nonsense musiceverywhere close then why markets , clinics open , close all\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "u look like those chao ah tiong\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "they better don ’ t breed\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "if you wan na know why the older generation calls us snowflake ' or the strawberry generation ' , simply read this post\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "mai gay siao la ccp ib . u only wan na hide behind reputable oversea chinese becos u know you caused big trouble worldwide and ppl are waiting hold u to account .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "coach red pill mentioned that women look at men and divide them into general 80 % of the men ( who got no chance in dating them ) and the top 20 % of men ( in terms of wealth , looks and fame ) . good to see these top 20 % men fall .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "parties is the gold standard . too arrogant to be seen to accept help .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i do n't a sutd nor support usa or china . can someone just nuke china for sake of humanity yes , i 'm a chinese single\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "ts do n't need brain if not using right ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "because they always tend to go for the `` exotic looking '' ones ( read - ugly ) , with extremely small eyes or exaggerated asian features . most asian guys wo n't even find them attractive anyway !\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "siao lang using imh wifi typing nonsense again\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "if you want to be a vigilante , then go join the police . why are people so nasty .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "lol ts use your brain more ... .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "what the **** ? ? ? you all really have brain issue , already say not ib .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "do us a favour take a long walk on a short pier bye you will not be missed\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "diam lah , talk so much cog\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "ah ah deserve it for not hiring sgporean .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i think your brain need a mask too\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` lau ama , mai kalan leh , knn tan limbei hoot li eh lau cb hor li si\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "ts and white dogs if you all have no ideas and suggestions to help singaporeans , ministers and the govt to do better . why do n't you shut the fark up ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i am christian but the sex is so good .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "fark this cb god then\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "u must be very free to be hantu and take people ’ s pictures behind their back . draw some balls for yourself .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "s/u this hgc really asking for it ... later parlemen say all cb dont allow smoking , then you see how many butts you need to eat from every shower\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you should ask your masters do n't act smart gey kiang smlj circuit breaker end up so much confusion . want to lock down , call it a lockdown , then you do n't have to blame people for not cooperating .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "open la~ u will earn more than him . can buy your own bags and own meals\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "foreign workers got infected totally due to fault of locals . these workers got infected due to careless behaviour of locals .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "since when he got drive to improve himself ? improve in what area ? at being useless ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "stay positive . hiv positive .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "who let the dogs out woof woof woof woof .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "behind closed doors say u so xia suay . come out say no singaporean will be left behind lolol .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you come here i pay you 600 u bend over so i can shove a broom up ur arse : x\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "body slam the fking aggressors pls\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "just nuke chinyan do it good . do n't be like japan so greedy ... just make sure all chinese in china back then was killed especially the leaders of that era ... no such problem will we have today .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "looks like the officer is having a hard time trying to educate a donkey . reserved one goody bag for the officer once everything is over give him the bag of goody .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "kudos to lta enforcement officer , cool and composed , you must know your work very well , you understand and calm each and everyone hearing your good advise . ( you didn ’ t realised monkey is recording you too , cause only small dd donkey do monkey business , if you know what i mean ) don ’ t be disheartened , you are doing a great job under hot sun . you went examples which the country most needed now . with this “ i salute you ” and i will salute you whenever i see you . “ well done ” and bubbles tea on me !\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "circuit breaker really accelerate their mental illness\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "why do n't send these comidita to the dormitorio as safe distancing ambassadors , pay them a salary but no mask to be put on less they pay $ 10am to wear one . then hopefully these idiots will learn a lesson or two .\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5045045045045045, 0.4, 0.6829268292682927, 0.6180555555555556)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcONLHFe6T6C",
        "outputId": "e88a7974-df4d-46f6-db09-d0d989e6c21d"
      },
      "source": [
        "train_and_evaluate_model(embeddings_dict=new_embeddings_dict, local=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding size:  1209600\n",
            "----------------Training input statistics----------------\n",
            "Preview of first 50 OOV words in test data:  ['10million', 'grabie', 'limpeh', 'unlucky..', 'edmwers', 'dormitories', 'la..china', 'Sporeans', 'uplorry', 'mask..', 'kwailan', 'northing', 'ourselves..', 'garhemen', \"'snowflakes\", 'COVID19', 'motherfcuker', 'grabbie..', 'covid', 'deglove', 'leeways', '100/day', 'Pappies', 'cut-paste', 'pedestral', 'China..', 'Limpeh', 'Covidiots', 'extramiles', 'pisai', 'satki', 'mah..', 'smoker…', 'Ptui', 'anti-chinese', 'leg..', 'enforcements', 'Groovelife', 'Kbkb', 'sh*t', 'COVID-19', 'Covid', 'PG/MG', 'sinkie', 'S/O', 'peepur', \"'strawberry\", 'attractive/can', 'again..', 'yayapapa']\n",
            "Total number of OOV words in test data:  56\n",
            "Max tokens in training data:  4948\n",
            "Average tokens:  75.06390318054257\n",
            "----------------------------------------------------------\n",
            "Epoch: 1/5... Step: 1069... Loss: 0.337703... Val Loss: 0.582029\n",
            "Validation loss decreased (inf --> 0.582029).  Saving model ...\n",
            "Epoch: 1/5... Step: 2138... Loss: 0.211306... Val Loss: 0.525061\n",
            "Validation loss decreased (0.582029 --> 0.525061).  Saving model ...\n",
            "Epoch: 2/5... Step: 3207... Loss: 0.118272... Val Loss: 0.634768\n",
            "Epoch: 2/5... Step: 4276... Loss: 0.012766... Val Loss: 0.647713\n",
            "Epoch: 3/5... Step: 5345... Loss: 0.267551... Val Loss: 0.636137\n",
            "Epoch: 3/5... Step: 6414... Loss: 0.147196... Val Loss: 0.696813\n",
            "Epoch: 4/5... Step: 7483... Loss: 0.082860... Val Loss: 0.629148\n",
            "Epoch: 4/5... Step: 8552... Loss: 0.139312... Val Loss: 0.611155\n",
            "Epoch: 5/5... Step: 9621... Loss: 0.259271... Val Loss: 0.785270\n",
            "Epoch: 5/5... Step: 10690... Loss: 0.003934... Val Loss: 0.739533\n",
            "##############################################################\n",
            "Test Statistics:\n",
            "Test F1 score: 0.6330935251798562\n",
            "Test recall: 0.6111111111111112\n",
            "Test precision: 0.6567164179104478\n",
            "Test accuracy: 0.6458333333333334\n",
            "##############################################################\n",
            "Label: 1.0, Prediction: 0.0\n",
            "maybe need to cane these metal teeth people then they will comply\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "aiyo it 's just diff cultures _UNKNOWN men are known to spoil their gfs and spend money on them like water like that , , they are literally brought up that way lol , , , tbh if he is not spending lavishly on you despite his high salary , why not u just communicate what u want ? ? ? why u want to post here let ppl flame u ? ? ? im sure you and ur bf stable enough to communicate your expectations right ? if buying a chanel bag for u makes u happy , i 'm sure he 'll go ahead and do it since he has the monetary means ? ? ? unless he is saving up for something then y'all can communicate about it like mature people ? ? ? there is no right or wrong la aiyo just communicate pls\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "do us a favour take a long walk on a short pier bye you will not be missed\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you know what is inside the greeny bag behind them ! ! not all but if you are _UNKNOWN 1 . unwashed zero maintenance in it . 2 . overnight spilled sauce or dry up soups . 3 . greeny unwashed uniform and helmets . 4 on the way slipper or old shoe if _UNKNOWN see one . 5 . last but not least all sort of things you can not imagine including tinny tiny little ones . enjoy your meal .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "put him to sleep . no need to talk\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "u look like those chao ah tiong\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you should ask your masters do n't act smart gey kiang smlj circuit breaker end up so much confusion . want to lock down , call it a lockdown , then you do n't have to blame people for not cooperating .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you come here i pay you 600 u bend over so i can shove a broom up ur arse : x\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "behind closed doors say u so xia suay . come out say no singaporean will be left behind lolol .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i am christian but the sex is so good .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "coach red pill mentioned that women look at men and divide them into general 80 % of the men ( who got no chance in dating them ) and the top 20 % of men ( in terms of wealth , looks and fame ) . good to see these top 20 % men fall .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN is the gold standard . too arrogant to be seen to accept help .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "if you wan na know why the older generation calls us _UNKNOWN ' or the _UNKNOWN generation ' , simply read this post\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN , erection still retain power . you guys really can not make it\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "singaporeans here talk big to hide the filthy conditions that existed in _UNKNOWN .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "you need professional help .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "circuit breaker really accelerate their mental illness\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "ts and white dogs if you all have no ideas and suggestions to help singaporeans , ministers and the govt to do better . why do n't you shut the fark up ?\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "jin _UNKNOWN man come lim peh clap for you\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "ah ah deserve it for not hiring _UNKNOWN .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "because they always tend to go for the `` exotic looking '' ones ( read - ugly ) , with extremely small eyes or exaggerated asian features . most asian guys wo n't even find them attractive anyway !\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN rings ... silicone . cheap , comfortable , durable and most importantly , it won ’ t _UNKNOWN your finger\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "why you laosai and vomit still drink milk tea ? try to refrain from dairy stuff . it creates nonsense with your tummy acid .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "a big shame on the public institutions who had failed her . so much so that a victim has to go out on display in social media to get her story heard ! !\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "body slam the fking aggressors pls\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "since when he got drive to improve himself ? improve in what area ? at being useless ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "open _UNKNOWN u will earn more than him . can buy your own bags and own meals\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "hence , may i appeal for all of you to stay in singapore , and avoid bringing back _UNKNOWN back into singapore upon your return ? this is for the sake of every singaporean , pr , and foreigner who responsibly chose to remain in singapore .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "life is unfair op . i 'm really sorry but this is how it is . if you are a scholar they own you\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "how you going to shut us up ? anyway , we are too good to take over govt . people like us who are honest , hardworking and skeptical are not suitable to be public servants .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "here a lot of those who complain richer than u , they are not loser haha\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "boy boy , do you challenge someone who gives you sound advice ? somemore that someone explained to you nicely but not being nasty . whether breaking law or not is another story . it is more about the personal moral and upbringing .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "diam lah , talk so much cog\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6330935251798562,\n",
              " 0.6111111111111112,\n",
              " 0.6567164179104478,\n",
              " 0.6458333333333334)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRyJfHxTDv-B",
        "outputId": "0ace6d00-d4ab-4b20-ede9-75ee64c2be34"
      },
      "source": [
        "from nltk.metrics import edit_distance\n",
        "\n",
        "def get_closest_word(word, dictionary):\n",
        "    min = np.inf\n",
        "    closest = \"\"\n",
        "    for s in dictionary:\n",
        "        dist = edit_distance(word, s)\n",
        "        if dist < min:\n",
        "            closest = s\n",
        "            min = dist\n",
        "    return closest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zswsuw35uV1Q"
      },
      "source": [
        "# Train Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7XqXLaruOzh",
        "outputId": "7a0e000a-58bb-4276-cb4e-a1844b3fb3fc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize  \n",
        "\n",
        "\n",
        "def main():\n",
        "    url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\n",
        "    train = pd.read_csv(url)\n",
        "    X = np.array(train['comment_text'])\n",
        "    y = np.array(train['toxic_label'])\n",
        "\n",
        "    size = X.shape[0]\n",
        "    X = X[:size]\n",
        "    y = y[:size]\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\n",
        "\n",
        "    model = LogisticRegression(random_state=0, solver='sag', max_iter=200)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_tf_idf_matrix = vectorizer.fit_transform(X_train)\n",
        "    model.fit( X_train_tf_idf_matrix, y_train)\n",
        "\n",
        "    # test your model\n",
        "    vectorizer_val = TfidfVectorizer(vocabulary=vectorizer.get_feature_names())\n",
        "    # vectorizer_val = TfidfVectorizer(vocabulary=vocab)\n",
        "    X_val_tf_idf_matrix = vectorizer_val.fit_transform(X_val)\n",
        "\n",
        "    y_pred = model.predict(X_val_tf_idf_matrix)\n",
        "    score = f1_score(y_val, y_pred, average='macro')\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    print('F1 score on validation = {}'.format(score))\n",
        "    print('accuracy = {}'.format(acc))\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "F1 score on validation = 0.8890783112960081\n",
            "accuracy = 0.8999064546304958\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}