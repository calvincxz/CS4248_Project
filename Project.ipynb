{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ffa21u-BtjFM",
        "ZMD5MO7bstul"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8AYewsFte9m"
      },
      "source": [
        "# Load pretrained word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h8mHA5ItdNH",
        "outputId": "232b10db-8d8d-42fe-baed-213f86bfa2d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW66B2Pdropx"
      },
      "source": [
        "import pickle\n",
        "import json\n",
        "\n",
        "# Define your own subdirectory \n",
        "subdir = \"Colab Notebooks/\"\n",
        "embedding_name = \"embeddings_dict.pkl\"\n",
        "# embedding_name = \"sms_glove.pkl\"\n",
        "\n",
        "path = f'/content/drive/MyDrive/{subdir}'\n",
        "embeddings_dict = pickle.load(open(f'{path}{embedding_name}', 'rb'))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffa21u-BtjFM"
      },
      "source": [
        "# Define LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb6dD99CWkGX"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "def get_weights(target_vocab, embeddings_dict, embedding_dim=100):\n",
        "    matrix_len = len(target_vocab)\n",
        "    weights_matrix = np.zeros((matrix_len, embedding_dim))\n",
        "    words_found = 0\n",
        "\n",
        "    for i, word in enumerate(target_vocab):\n",
        "        try: \n",
        "            weights_matrix[i] = embeddings_dict[word]\n",
        "            words_found += 1\n",
        "        except KeyError:\n",
        "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))  \n",
        "\n",
        "    print(\"% of OOV words (not found in embedding): \", round(1.0 - words_found/matrix_len, 3) * 100)\n",
        "    return torch.tensor(weights_matrix)\n",
        "\n",
        "def create_emb_layer(weights_matrix, trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "    # set trainable for weights\n",
        "    emb_layer.weight.requires_grad = trainable\n",
        "      \n",
        "    return emb_layer, num_embeddings, embedding_dim  \n",
        "\n",
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, target_vocab, embeddings_dict, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Learn new word embedding\n",
        "        # vocab_size = len(target_vocab)\n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Use pretrained word embedding\n",
        "        weights_matrix = get_weights(target_vocab, embeddings_dict, embedding_dim)\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, trainable=True)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # out = self.dropout(lstm_out)\n",
        "        out = self.fc1(lstm_out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMD5MO7bstul"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9irSayVjR6q"
      },
      "source": [
        "# Pad/truncate sentence to fixed length\n",
        "# https://arxiv.org/abs/1903.07288 pad zeros in front\n",
        "def pad_input(sentences, seq_len=100):\n",
        "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
        "    for i, tokens in enumerate(sentences):\n",
        "        if len(tokens) > seq_len:\n",
        "            features[i] = np.array(tokens)[:seq_len]\n",
        "        elif len(tokens) > 0:\n",
        "            features[i, -len(tokens):] = np.array(tokens)\n",
        "    return features\n",
        "\n",
        "def preprocess_input(X, y, seq_len=100):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\n",
        "\n",
        "    words = Counter() \n",
        "\n",
        "    for i, sentence in enumerate(X_train):\n",
        "        # The sentences will be stored as a list of words/tokens\n",
        "        X_train[i] = []\n",
        "        for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
        "            words.update([word.lower()])  # Converting all the words to lowercase\n",
        "            X_train[i].append(word)\n",
        "    \n",
        "    words = {k:v for k,v in words.items()}\n",
        "    words = sorted(words, key=words.get, reverse=True)\n",
        "    # Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
        "    words = ['_PAD', '_UNKNOWN'] + words\n",
        "\n",
        "    # Dictionaries to store the word to index mappings and vice versa\n",
        "    word2idx = {o:i for i,o in enumerate(words)}\n",
        "    idx2word = {i:o for i,o in enumerate(words)}\n",
        "\n",
        "    for i, sentence in enumerate(X_train):\n",
        "        X_train[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
        "\n",
        "    for i, sentence in enumerate(X_val):\n",
        "        X_val[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\n",
        "\n",
        "    for i, sentence in enumerate(X_test):\n",
        "        X_test[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\n",
        "    print(\"Training input statistics\")\n",
        "    print(\"Max tokens: \", np.max([len(x) for x in X_train]))\n",
        "    print(\"Min tokens: \", np.min([len(x) for x in X_train]))\n",
        "    print(\"Average tokens: \", np.mean([len(x) for x in X_train]))\n",
        "\n",
        "    train_sentences = pad_input(X_train, seq_len)\n",
        "    val_sentences = pad_input(X_val, seq_len)\n",
        "    test_sentences = pad_input(X_test, seq_len)\n",
        "\n",
        "    return train_sentences, val_sentences, test_sentences, y_train, y_val, y_test, words, idx2word"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qs3MAd1szWS"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pluu00U1PXCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e6929a-b98b-4b9b-cae1-530e1fbd1a56"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize  \n",
        "\n",
        "\n",
        "def main():\n",
        "    url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\n",
        "    train = pd.read_csv(url)\n",
        "    X = np.array(train['comment_text'])\n",
        "    y = np.array(train['toxic_label'])\n",
        "\n",
        "    # reduce size for faster training\n",
        "    size = X.shape[0]\n",
        "    # size = 40000\n",
        "\n",
        "    X = X[:size]\n",
        "    y = y[:size]\n",
        "\n",
        "    train_sentences, val_sentences, test_sentences, \\\n",
        "    y_train, y_val, y_test, vocab, idx2word = preprocess_input(X, y, seq_len=200)\n",
        "\n",
        "    train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(y_train))\n",
        "    val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(y_val))\n",
        "    test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(y_test))\n",
        "\n",
        "    batch_size = 256\n",
        "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "    val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "    output_size = 1\n",
        "    embedding_dim = len(list(embeddings_dict.values())[0])\n",
        "    hidden_dim = 256\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    model = SentimentNet(vocab, embeddings_dict, output_size, embedding_dim, hidden_dim, n_layers=2, drop_prob=0.2)\n",
        "    model.to(device)\n",
        "\n",
        "    lr=0.0005\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n",
        "\n",
        "    epochs = 8\n",
        "    counter = 0\n",
        "    print_every = int(0.5 * len(y_train) / batch_size)\n",
        "    clip = 5\n",
        "    valid_loss_min = np.Inf\n",
        "    best_model_so_far = None\n",
        "\n",
        "    # Set seed\n",
        "    torch.manual_seed(1)\n",
        "\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "        h = model.init_hidden(batch_size)\n",
        "        \n",
        "        for inputs, labels in train_loader:\n",
        "            counter += 1\n",
        "            h = tuple([e.data for e in h])\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            model.zero_grad()\n",
        "            output, h = model(inputs, h)\n",
        "            loss = criterion(output.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "            # lr_scheduler.step()\n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "                val_h = model.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                model.eval()\n",
        "                for val_input, val_label in val_loader:\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    val_input, val_label = val_input.to(device), val_label.to(device)\n",
        "                    out, val_h = model(val_input, val_h)\n",
        "                    val_loss = criterion(out.squeeze(), val_label.float())\n",
        "                    val_losses.append(val_loss.item())\n",
        "                    \n",
        "                model.train()\n",
        "                print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "                if np.mean(val_losses) <= valid_loss_min:\n",
        "                    # torch.save(model.state_dict(), './state_dict.pt')\n",
        "                    best_model_so_far = copy.deepcopy(model)\n",
        "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                    valid_loss_min = np.mean(val_losses)\n",
        "\n",
        "    test_losses = []\n",
        "    h = model.init_hidden(batch_size)\n",
        "    test_inputs = np.array([])\n",
        "    outputs = np.array([])\n",
        "    test_labels = np.array([])\n",
        "\n",
        "    # load the best model which was saved previously\n",
        "    model =  best_model_so_far\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            h = tuple([each.data for each in h])\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            output, h = model(inputs, h)\n",
        "            test_loss = criterion(output.squeeze(), labels.float())\n",
        "            test_losses.append(test_loss.item())\n",
        "            pred = torch.round(output.squeeze())\n",
        "\n",
        "            if len(test_inputs) == 0:\n",
        "                test_inputs = inputs.cpu().numpy()\n",
        "            else:\n",
        "                test_inputs = np.concatenate([test_inputs, inputs.cpu().numpy()])\n",
        "            outputs = np.concatenate([outputs, pred.cpu().numpy()])\n",
        "            test_labels = np.concatenate([test_labels, labels.cpu().numpy()])\n",
        "        \n",
        "        f1score = f1_score(np.array(test_labels), outputs)\n",
        "        recall = recall_score(np.array(test_labels), outputs)\n",
        "        precision = precision_score(np.array(test_labels), outputs)\n",
        "        accuracy = accuracy_score(np.array(test_labels), outputs)\n",
        "        print(f\"Test F1 score: {f1score}\")\n",
        "        print(f\"Test recall: {recall}\")\n",
        "        print(f\"Test precision: {precision}\")\n",
        "        print(f\"Test accuracy: {accuracy}\")\n",
        "        print(\"##############################################################\")\n",
        "        # Print some wrong predictions\n",
        "        for i, input in enumerate(test_inputs[:100]):\n",
        "            if outputs[i] != test_labels[i]:\n",
        "              input_no_pad = input[input != 0]\n",
        "              print(f\"Label: {test_labels[i]}, Prediction: {outputs[i]}\")\n",
        "              print(\" \".join([idx2word[i] for i in input_no_pad]))\n",
        "              print()\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Training input statistics\n",
            "Max tokens:  4948\n",
            "Min tokens:  2\n",
            "Average tokens:  75.06390318054257\n",
            "cuda\n",
            "% of OOV words (not found in embedding):  50.1\n",
            "Epoch: 1/8... Step: 66... Loss: 0.383569... Val Loss: 0.362770\n",
            "Validation loss decreased (inf --> 0.362770).  Saving model ...\n",
            "Epoch: 1/8... Step: 132... Loss: 0.329196... Val Loss: 0.351799\n",
            "Validation loss decreased (0.362770 --> 0.351799).  Saving model ...\n",
            "Epoch: 2/8... Step: 198... Loss: 0.249144... Val Loss: 0.254883\n",
            "Validation loss decreased (0.351799 --> 0.254883).  Saving model ...\n",
            "Epoch: 2/8... Step: 264... Loss: 0.294425... Val Loss: 0.252922\n",
            "Validation loss decreased (0.254883 --> 0.252922).  Saving model ...\n",
            "Epoch: 3/8... Step: 330... Loss: 0.299359... Val Loss: 0.244083\n",
            "Validation loss decreased (0.252922 --> 0.244083).  Saving model ...\n",
            "Epoch: 3/8... Step: 396... Loss: 0.216580... Val Loss: 0.219636\n",
            "Validation loss decreased (0.244083 --> 0.219636).  Saving model ...\n",
            "Epoch: 4/8... Step: 462... Loss: 0.268980... Val Loss: 0.237147\n",
            "Epoch: 4/8... Step: 528... Loss: 0.208200... Val Loss: 0.211521\n",
            "Validation loss decreased (0.219636 --> 0.211521).  Saving model ...\n",
            "Epoch: 5/8... Step: 594... Loss: 0.179565... Val Loss: 0.204144\n",
            "Validation loss decreased (0.211521 --> 0.204144).  Saving model ...\n",
            "Epoch: 5/8... Step: 660... Loss: 0.216222... Val Loss: 0.213595\n",
            "Epoch: 6/8... Step: 726... Loss: 0.154930... Val Loss: 0.199001\n",
            "Validation loss decreased (0.204144 --> 0.199001).  Saving model ...\n",
            "Epoch: 6/8... Step: 792... Loss: 0.166020... Val Loss: 0.204330\n",
            "Epoch: 7/8... Step: 858... Loss: 0.175209... Val Loss: 0.199023\n",
            "Epoch: 7/8... Step: 924... Loss: 0.150238... Val Loss: 0.209674\n",
            "Epoch: 8/8... Step: 990... Loss: 0.154016... Val Loss: 0.230685\n",
            "Epoch: 8/8... Step: 1056... Loss: 0.165381... Val Loss: 0.206046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test F1 score: 0.8986666666666666\n",
            "Test recall: 0.8833551769331586\n",
            "Test precision: 0.9145183175033921\n",
            "Test accuracy: 0.92578125\n",
            "##############################################################\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i guess it 's easy to dish out criticism but you ca n't take it . do n't blame me for the fact that coldplay is a collection of _UNKNOWN music illiterates , championed by a badly diseased western media . i know what makes for music quality and coldplay is certainly anything but .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "quit censoring or get _UNKNOWN removing the porn scandal background is against wiki policies . ==\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you are not targeting the players you are targeting the club ! ! ! you wo n't change my mind about it . you just want me to give you attention . goodbye al\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "stop with the warnings ill just be back lol lol lol lol lol lol u started it sssss\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` the `` '' jobber '' '' had a name . rory fox to be exact . good to see laziness still exists . poor _UNKNOWN what a a _UNKNOWN ''\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i do n't think he 's gay , but he may be in a relationship with a guy . what 's the big deal ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` here 's something for you to report you have more poop in your pants . vandals are people who disrupt wikipedia , like people who get bent out of shape over comments made on talk pages for ip address , rather than focusing on something ... important . they write condescending comments and put images of exclamation points in them and avoid communicating with other people like they 're people , instead they use an impersonal language of _UNKNOWN warnings sent like notices from bill collectors ( who would , hypothetically also have poopie pants ) . you may feel like you 're being very noble by reporting this kind of comments to aiv , and that when someone communicates with you in a way that you 're not _UNKNOWN with or that makes you uncomfortable that it 's `` '' vandalism '' '' and `` '' personal attacks . '' '' but it 's really not , it 's just an emerging bureaucratic culture preventing wikipedia from becoming a true form of social networking and collaboration . anyway , go ahead , go nuts , pick on people different from you and have me banned . just remember\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i 'm sure you eat alot of tube steak\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zswsuw35uV1Q"
      },
      "source": [
        "# Train Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7XqXLaruOzh",
        "outputId": "0070b795-51a6-4c81-82d9-8e8d0a70f043"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize  \n",
        "\n",
        "\n",
        "def main():\n",
        "    url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\n",
        "    train = pd.read_csv(url)\n",
        "    X = np.array(train['comment_text'])\n",
        "    y = np.array(train['toxic_label'])\n",
        "\n",
        "    size = 10000\n",
        "    X = X[:size]\n",
        "    y = y[:size]\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\n",
        "\n",
        "    model = LogisticRegression(random_state=0, solver='sag', max_iter=200)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_tf_idf_matrix = vectorizer.fit_transform(X_train)\n",
        "    model.fit( X_train_tf_idf_matrix, y_train)\n",
        "\n",
        "    # test your model\n",
        "    vectorizer_val = TfidfVectorizer(vocabulary=vectorizer.get_feature_names())\n",
        "    # vectorizer_val = TfidfVectorizer(vocabulary=vocab)\n",
        "    X_val_tf_idf_matrix = vectorizer_val.fit_transform(X_val)\n",
        "\n",
        "    y_pred = model.predict(X_val_tf_idf_matrix)\n",
        "    score = f1_score(y_val, y_pred, average='macro')\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    print('F1 score on validation = {}'.format(score))\n",
        "    print('accuracy = {}'.format(acc))\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "F1 score on validation = 0.7162126068376068\n",
            "accuracy = 0.932\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}