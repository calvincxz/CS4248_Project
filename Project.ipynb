{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ffa21u-BtjFM",
        "ZMD5MO7bstul"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8AYewsFte9m"
      },
      "source": [
        "# Load pretrained word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h8mHA5ItdNH",
        "outputId": "d4e69fce-0796-41dd-afbb-59986f1ed26a"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW66B2Pdropx"
      },
      "source": [
        "import pickle\r\n",
        "import json\r\n",
        "\r\n",
        "# Define your own subdirectory \r\n",
        "subdir = \"Colab Notebooks/\"\r\n",
        "\r\n",
        "path = f'/content/drive/MyDrive/{subdir}'\r\n",
        "embeddings_dict = pickle.load(open(f'{path}/embeddings_dict.pkl', 'rb'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffa21u-BtjFM"
      },
      "source": [
        "# Define LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb6dD99CWkGX"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "else:\r\n",
        "    device = torch.device(\"cpu\")\r\n",
        "\r\n",
        "def get_weights(target_vocab, embeddings_dict, embedding_dim=100):\r\n",
        "    matrix_len = len(target_vocab)\r\n",
        "    weights_matrix = np.zeros((matrix_len, embedding_dim))\r\n",
        "    words_found = 0\r\n",
        "\r\n",
        "    for i, word in enumerate(target_vocab):\r\n",
        "        try: \r\n",
        "            weights_matrix[i] = embeddings_dict[word]\r\n",
        "            words_found += 1\r\n",
        "        except KeyError:\r\n",
        "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))  \r\n",
        "\r\n",
        "    print(\"Fraction of vocab words found in word embedding: \", words_found/matrix_len)\r\n",
        "    return torch.tensor(weights_matrix)\r\n",
        "\r\n",
        "def create_emb_layer(weights_matrix, trainable=False):\r\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape\r\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\r\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix})\r\n",
        "    # set trainable for weights\r\n",
        "    emb_layer.weight.requires_grad = trainable\r\n",
        "      \r\n",
        "    return emb_layer, num_embeddings, embedding_dim  \r\n",
        "\r\n",
        "class SentimentNet(nn.Module):\r\n",
        "    def __init__(self, target_vocab, embeddings_dict, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\r\n",
        "        super(SentimentNet, self).__init__()\r\n",
        "        self.output_size = output_size\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "\r\n",
        "        # Learn new word embedding\r\n",
        "        # vocab_size = len(target_vocab)\r\n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "\r\n",
        "        # Use pretrained word embedding\r\n",
        "        weights_matrix = get_weights(target_vocab, embeddings_dict, embedding_dim)\r\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, trainable=True)\r\n",
        "        \r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\r\n",
        "        self.dropout = nn.Dropout(drop_prob)\r\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\r\n",
        "        self.sigmoid = nn.Sigmoid()\r\n",
        "        \r\n",
        "    def forward(self, x, hidden):\r\n",
        "        batch_size = x.size(0)\r\n",
        "        x = x.long()\r\n",
        "        embeds = self.embedding(x)\r\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\r\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\r\n",
        "        \r\n",
        "        out = self.dropout(lstm_out)\r\n",
        "        out = self.fc(out)\r\n",
        "        out = self.sigmoid(out)\r\n",
        "        \r\n",
        "        out = out.view(batch_size, -1)\r\n",
        "        out = out[:,-1]\r\n",
        "        return out, hidden\r\n",
        "    \r\n",
        "    def init_hidden(self, batch_size):\r\n",
        "        weight = next(self.parameters()).data\r\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\r\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\r\n",
        "        return hidden"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMD5MO7bstul"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9irSayVjR6q"
      },
      "source": [
        "# Pad/truncate sentence to fixed length\r\n",
        "# https://arxiv.org/abs/1903.07288 pad zeros in front\r\n",
        "def pad_input(sentences, seq_len=100):\r\n",
        "    features = np.zeros((len(sentences), seq_len), dtype=int)\r\n",
        "    for i, tokens in enumerate(sentences):\r\n",
        "        if len(tokens) > seq_len:\r\n",
        "            features[i] = np.array(tokens)[:seq_len]\r\n",
        "        elif len(tokens) > 0:\r\n",
        "            features[i, -len(tokens):] = np.array(tokens)\r\n",
        "    return features\r\n",
        "\r\n",
        "def preprocess_input(X, y, seq_len=100):\r\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\r\n",
        "\r\n",
        "    words = Counter() \r\n",
        "\r\n",
        "    for i, sentence in enumerate(X_train):\r\n",
        "        # The sentences will be stored as a list of words/tokens\r\n",
        "        X_train[i] = []\r\n",
        "        for word in nltk.word_tokenize(sentence):  # Tokenizing the words\r\n",
        "            words.update([word.lower()])  # Converting all the words to lowercase\r\n",
        "            X_train[i].append(word)\r\n",
        "    \r\n",
        "    words = {k:v for k,v in words.items()}\r\n",
        "    # # Sorting the words according to the number of appearances, with the most common word being first\r\n",
        "    words = sorted(words, key=words.get, reverse=True)\r\n",
        "    # Adding padding and unknown to our vocabulary so that they will be assigned an index\r\n",
        "    words = ['_PAD', '_UNKNOWN'] + words\r\n",
        "\r\n",
        "    # Dictionaries to store the word to index mappings and vice versa\r\n",
        "    word2idx = {o:i for i,o in enumerate(words)}\r\n",
        "    idx2word = {i:o for i,o in enumerate(words)}\r\n",
        "\r\n",
        "    for i, sentence in enumerate(X_train):\r\n",
        "        X_train[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\r\n",
        "\r\n",
        "    for i, sentence in enumerate(X_val):\r\n",
        "        X_val[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\r\n",
        "\r\n",
        "    for i, sentence in enumerate(X_test):\r\n",
        "        X_test[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\r\n",
        "\r\n",
        "    print(\"Max tokens: \", np.max([len(x) for x in X_train]))\r\n",
        "    print(\"Min tokens: \", np.min([len(x) for x in X_train]))\r\n",
        "    print(\"Average tokens: \", np.mean([len(x) for x in X_train]))\r\n",
        "\r\n",
        "    train_sentences = pad_input(X_train)\r\n",
        "    val_sentences = pad_input(X_val)\r\n",
        "    test_sentences = pad_input(X_test)\r\n",
        "\r\n",
        "    return train_sentences, val_sentences, test_sentences, y_train, y_val, y_test, words, idx2word"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qs3MAd1szWS"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pluu00U1PXCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675b82e9-f2be-4b05-c069-4b0c19aa64bb"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from scipy.sparse import csr_matrix\r\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "import math\r\n",
        "from collections import Counter\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk import word_tokenize  \r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\r\n",
        "    train = pd.read_csv(url)\r\n",
        "    X = np.array(train['comment_text'])\r\n",
        "    y = np.array(train['toxic_label'])\r\n",
        "\r\n",
        "    # reduce size for faster training\r\n",
        "    size = X.shape[0]\r\n",
        "    # size = 40000\r\n",
        "\r\n",
        "    X = X[:size]\r\n",
        "    y = y[:size]\r\n",
        "\r\n",
        "    train_sentences, val_sentences, test_sentences, \\\r\n",
        "    y_train, y_val, y_test, vocab, idx2word = preprocess_input(X, y)\r\n",
        "\r\n",
        "    train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(y_train))\r\n",
        "    val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(y_val))\r\n",
        "    test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(y_test))\r\n",
        "\r\n",
        "    batch_size = 200\r\n",
        "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
        "    val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
        "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
        "\r\n",
        "    output_size = 1\r\n",
        "    embedding_dim = 100\r\n",
        "    hidden_dim = 256\r\n",
        "    n_layers = 2\r\n",
        "    vocab_size = len(vocab)\r\n",
        "\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        device = torch.device(\"cuda\")\r\n",
        "    else:\r\n",
        "        device = torch.device(\"cpu\")\r\n",
        "    print(device)\r\n",
        "\r\n",
        "    model = SentimentNet(vocab, embeddings_dict, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.3)\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "    lr=0.001\r\n",
        "    criterion = nn.BCELoss()\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\r\n",
        "\r\n",
        "    epochs = 5\r\n",
        "    counter = 0\r\n",
        "    print_every = int(0.5 * len(y_train) / batch_size)\r\n",
        "    clip = 5\r\n",
        "    valid_loss_min = np.Inf\r\n",
        "\r\n",
        "    # Set seed\r\n",
        "    torch.manual_seed(1)\r\n",
        "\r\n",
        "    model.train()\r\n",
        "    for i in range(epochs):\r\n",
        "        h = model.init_hidden(batch_size)\r\n",
        "        \r\n",
        "        for inputs, labels in train_loader:\r\n",
        "            counter += 1\r\n",
        "            h = tuple([e.data for e in h])\r\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "            model.zero_grad()\r\n",
        "            output, h = model(inputs, h)\r\n",
        "            loss = criterion(output.squeeze(), labels.float())\r\n",
        "            loss.backward()\r\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "            optimizer.step()\r\n",
        "            # lr_scheduler.step()\r\n",
        "            \r\n",
        "            if counter % print_every == 0:\r\n",
        "                val_h = model.init_hidden(batch_size)\r\n",
        "                val_losses = []\r\n",
        "                model.eval()\r\n",
        "                for val_input, val_label in val_loader:\r\n",
        "                    val_h = tuple([each.data for each in val_h])\r\n",
        "                    val_input, val_label = val_input.to(device), val_label.to(device)\r\n",
        "                    out, val_h = model(val_input, val_h)\r\n",
        "                    val_loss = criterion(out.squeeze(), val_label.float())\r\n",
        "                    val_losses.append(val_loss.item())\r\n",
        "                    \r\n",
        "                model.train()\r\n",
        "                print(\"Epoch: {}/{}...\".format(i+1, epochs),\r\n",
        "                      \"Step: {}...\".format(counter),\r\n",
        "                      \"Loss: {:.6f}...\".format(loss.item()),\r\n",
        "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\r\n",
        "                if np.mean(val_losses) <= valid_loss_min:\r\n",
        "                    # torch.save(model.state_dict(), './state_dict.pt')\r\n",
        "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\r\n",
        "                    valid_loss_min = np.mean(val_losses)\r\n",
        "\r\n",
        "    test_losses = []\r\n",
        "    h = model.init_hidden(batch_size)\r\n",
        "    test_inputs = np.array([])\r\n",
        "    outputs = np.array([])\r\n",
        "    test_labels = np.array([])\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        for inputs, labels in test_loader:\r\n",
        "            h = tuple([each.data for each in h])\r\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "            output, h = model(inputs, h)\r\n",
        "            test_loss = criterion(output.squeeze(), labels.float())\r\n",
        "            test_losses.append(test_loss.item())\r\n",
        "            pred = torch.round(output.squeeze())\r\n",
        "\r\n",
        "            if len(test_inputs) == 0:\r\n",
        "                test_inputs = inputs.cpu().numpy()\r\n",
        "            else:\r\n",
        "                test_inputs = np.concatenate([test_inputs, inputs.cpu().numpy()])\r\n",
        "            outputs = np.concatenate([outputs, pred.cpu().numpy()])\r\n",
        "            test_labels = np.concatenate([test_labels, labels.cpu().numpy()])\r\n",
        "        \r\n",
        "        f1score = f1_score(np.array(test_labels), outputs)\r\n",
        "        recall = recall_score(np.array(test_labels), outputs)\r\n",
        "        precision = precision_score(np.array(test_labels), outputs)\r\n",
        "        accuracy = accuracy_score(np.array(test_labels), outputs)\r\n",
        "        print(f\"Test F1 score: {f1score}\")\r\n",
        "        print(f\"Test recall: {recall}\")\r\n",
        "        print(f\"Test precision: {precision}\")\r\n",
        "        print(f\"Test accuracy: {accuracy}\")\r\n",
        "        print(\"##############################################################\")\r\n",
        "        # Print some wrong predictions\r\n",
        "        for i, input in enumerate(test_inputs[:100]):\r\n",
        "            if outputs[i] != test_labels[i]:\r\n",
        "              input_no_pad = input[input != 0]\r\n",
        "              print(f\"Label: {test_labels[i]}, Prediction: {outputs[i]}\")\r\n",
        "              print(\" \".join([idx2word[i] for i in input_no_pad]))\r\n",
        "              print()\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    main()\r\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Max tokens:  4948\n",
            "Min tokens:  2\n",
            "Average tokens:  75.75225\n",
            "cuda\n",
            "Fraction of vocab words found in word embedding:  0.5043308817413827\n",
            "Epoch: 1/5... Step: 80... Loss: 0.296390... Val Loss: 0.301030\n",
            "Validation loss decreased (inf --> 0.301030).  Saving model ...\n",
            "Epoch: 1/5... Step: 160... Loss: 0.341361... Val Loss: 0.250103\n",
            "Validation loss decreased (0.301030 --> 0.250103).  Saving model ...\n",
            "Epoch: 2/5... Step: 240... Loss: 0.295451... Val Loss: 0.229450\n",
            "Validation loss decreased (0.250103 --> 0.229450).  Saving model ...\n",
            "Epoch: 2/5... Step: 320... Loss: 0.295802... Val Loss: 0.229338\n",
            "Validation loss decreased (0.229450 --> 0.229338).  Saving model ...\n",
            "Epoch: 3/5... Step: 400... Loss: 0.262506... Val Loss: 0.217625\n",
            "Validation loss decreased (0.229338 --> 0.217625).  Saving model ...\n",
            "Epoch: 3/5... Step: 480... Loss: 0.218398... Val Loss: 0.207886\n",
            "Validation loss decreased (0.217625 --> 0.207886).  Saving model ...\n",
            "Epoch: 4/5... Step: 560... Loss: 0.239918... Val Loss: 0.207459\n",
            "Validation loss decreased (0.207886 --> 0.207459).  Saving model ...\n",
            "Epoch: 4/5... Step: 640... Loss: 0.219673... Val Loss: 0.226618\n",
            "Epoch: 5/5... Step: 720... Loss: 0.152179... Val Loss: 0.204471\n",
            "Validation loss decreased (0.207459 --> 0.204471).  Saving model ...\n",
            "Epoch: 5/5... Step: 800... Loss: 0.114174... Val Loss: 0.206965\n",
            "Test F1 score: 0.864030131826742\n",
            "Test recall: 0.8650075414781297\n",
            "Test precision: 0.8630549285176825\n",
            "Test accuracy: 0.90975\n",
            "##############################################################\n",
            "Label: 0.0, Prediction: 1.0\n",
            "yes , thank you very much ! you 're my favorite sysop . the way you ignore the questions you ca n't answer is very commendable . also , when you did answer the one question , you did not use facts but again , your gut feeling ! i bow to your infinite wisdom , o mighty one .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "do so sexually with me\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "warning me if you dare to warn me i dont really give a rip . i will delete them and keep on deleteing them . i am making a new account so _UNKNOWN\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "`` if they have problems with the items being placed by others , and they insist on crying about `` '' go to the talk page '' '' then here i am : post your goddamn complaints rather than just _UNKNOWN reverting people like arrow ! ``\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "removing warnings is considered anger that other people type _UNKNOWN on my discussion page . just for your info .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you are an obsessive religious and nationalistic fanatic that vandalises the article on nicosia constantly . i have not even mentioned the word greek claim and i have respected your monuments , that are not even yours and are _UNKNOWN greek or gothic and you present those twisting historical facts as turkish . that did not suffice . people like you should not be allowed to present encyclopedic material as you are clearly unable to be unbiased .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` _UNKNOWN as the idea of the `` '' _UNKNOWN relationship '' '' commonly exists and is discussed and portrayed in works of fiction the subject deserves an article . this article , however , is possibly one of the stupidest articles in all of wikipedia ( and i just finished reading though the `` '' sex moves '' '' category ) . ''\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "do n't revert me _UNKNOWN ! as a conservative i think we need to _UNKNOWN senator kerry - - i think we need to add a section applauding kerry 's denigrating and insulting cheap shot insults directed at our brave soldiers in iraq . - - after all , senator kerry just _UNKNOWN that conservatives will maintain control of both the house and _UNKNOWN this year with his despicable comments . - - my deepest condolences to kerry apologists and liberal editors at wikipedia ( in other words , at least 90 % of you . ) - - in\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zswsuw35uV1Q"
      },
      "source": [
        "# Train Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7XqXLaruOzh",
        "outputId": "0070b795-51a6-4c81-82d9-8e8d0a70f043"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from scipy.sparse import csr_matrix\r\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "import math\r\n",
        "from collections import Counter\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk import word_tokenize  \r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\r\n",
        "    train = pd.read_csv(url)\r\n",
        "    X = np.array(train['comment_text'])\r\n",
        "    y = np.array(train['toxic_label'])\r\n",
        "\r\n",
        "    size = 10000\r\n",
        "    X = X[:size]\r\n",
        "    y = y[:size]\r\n",
        "\r\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\r\n",
        "\r\n",
        "    model = LogisticRegression(random_state=0, solver='sag', max_iter=200)\r\n",
        "    vectorizer = TfidfVectorizer()\r\n",
        "    X_train_tf_idf_matrix = vectorizer.fit_transform(X_train)\r\n",
        "    model.fit( X_train_tf_idf_matrix, y_train)\r\n",
        "\r\n",
        "    # test your model\r\n",
        "    vectorizer_val = TfidfVectorizer(vocabulary=vectorizer.get_feature_names())\r\n",
        "    # vectorizer_val = TfidfVectorizer(vocabulary=vocab)\r\n",
        "    X_val_tf_idf_matrix = vectorizer_val.fit_transform(X_val)\r\n",
        "\r\n",
        "    y_pred = model.predict(X_val_tf_idf_matrix)\r\n",
        "    score = f1_score(y_val, y_pred, average='macro')\r\n",
        "    acc = accuracy_score(y_val, y_pred)\r\n",
        "    print('F1 score on validation = {}'.format(score))\r\n",
        "    print('accuracy = {}'.format(acc))\r\n",
        " \r\n",
        "if __name__ == \"__main__\":\r\n",
        "    main()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "F1 score on validation = 0.7162126068376068\n",
            "accuracy = 0.932\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}