{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ffa21u-BtjFM",
        "ZMD5MO7bstul",
        "i6jR6yjifjbt"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78dGhMtdvq81"
      },
      "source": [
        "# Data scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLlRx3wc3Qww",
        "outputId": "14f50af8-f78a-49cc-e6f3-4fb6121a4302"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jANQNZG8nUxR"
      },
      "source": [
        "import json\n",
        "\n",
        "subdir1 = \"CS4248 Project/ntuconfessions_data.json\"\n",
        "subdir2 = \"CS4248 Project/ntu.json\"\n",
        "\n",
        "def load_file(subdir):\n",
        "    path = f'/content/drive/MyDrive/{subdir}'\n",
        "    data = json.load(open(f'{path}', 'rb'))\n",
        "    return data\n",
        "    \n",
        "data1 = load_file(subdir1)\n",
        "data2 = load_file(subdir2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6YDQu2DnyLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fc0009-6638-4358-e89c-ecf882a0600d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_data(data):\n",
        "    posts = data[0][\"posts\"]\n",
        "    visited_urls = set()\n",
        "    post_contents = []\n",
        "    post_comments = []\n",
        "    for post in posts:\n",
        "        if post[\"postUrl\"] in visited_urls:\n",
        "            continue\n",
        "            \n",
        "        visited_urls.add(post[\"postUrl\"])\n",
        "        post_text = post[\"postText\"]\n",
        "        # remove hyperlink at end of post\n",
        "        # truncated_text = post_text[:-53] # for nus\n",
        "        truncated_text = post_text[:-47] # for ntu\n",
        "        # truncated_text = post_text\n",
        "        post_contents.append(truncated_text)\n",
        "\n",
        "        for comment in post[\"postComments\"][\"comments\"]:\n",
        "            comment_text = comment[\"text\"]\n",
        "            post_comments.append(comment_text)\n",
        "    return post_contents, post_comments\n",
        "\n",
        "x1, x2 = process_data(data1)\n",
        "x3, x4 = process_data(data2)\n",
        "print(len(x3), len(x4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "652 3039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6OkZKim3iN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9e83ad-95c4-4b4d-ec85-ce6b839aeb49"
      },
      "source": [
        "post_contents = x1 + x2\n",
        "post_comments = x3 + x4\n",
        "print(len(post_contents), len(post_comments))\n",
        "\n",
        "df1 = pd.DataFrame (post_contents,columns=['post_contents'])\n",
        "df2 = pd.DataFrame (post_comments,columns=['post_comments'])\n",
        "\n",
        "df1.to_csv(r'ntu_posts.csv', index = False, header=True)\n",
        "df2.to_csv(r'ntu_comments.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "482 3691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8AYewsFte9m"
      },
      "source": [
        "# Load pretrained word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h8mHA5ItdNH",
        "outputId": "001092c2-0882-4f87-86b6-2ff1101f8a69"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW66B2Pdropx"
      },
      "source": [
        "import pickle\n",
        "import json\n",
        "\n",
        "# Define your own subdirectory \n",
        "subdir = \"CS4248 Project/Data\"\n",
        "path = f'/content/drive/MyDrive/{subdir}'\n",
        "\n",
        "twitter_embedding_name = \"embeddings_dict.pkl\"\n",
        "wikipedia_embedding_name = \"embeddings_dict_wikipedia_100d.pkl\"\n",
        "finetune_embedding_name = \"sms_glove.pkl\"\n",
        "\n",
        "twitter_embeddings_dict = pickle.load(open(f'{path}{twitter_embedding_name}', 'rb'))\n",
        "wikipedia_embeddings_dict = pickle.load(open(f'{path}{wikipedia_embedding_name}', 'rb'))\n",
        "\n",
        "# update embedding\n",
        "# new_embeddings_dict = pickle.load(open(f'{path}{finetune_embedding_name}', 'rb'))\n",
        "# embeddings_dict.update(new_embeddings_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffa21u-BtjFM"
      },
      "source": [
        "# Define LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb6dD99CWkGX"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import textwrap\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "def get_weights(target_vocab, embeddings_dict, embedding_dim=100):\n",
        "    matrix_len = len(target_vocab)\n",
        "    weights_matrix = np.zeros((matrix_len, embedding_dim))\n",
        "    words_found = 0\n",
        "    oov_words = []\n",
        "\n",
        "    for i, word in enumerate(target_vocab):\n",
        "        try: \n",
        "            weights_matrix[i] = embeddings_dict[word]\n",
        "            words_found += 1\n",
        "        except KeyError:\n",
        "            oov_words.append(word)\n",
        "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))  \n",
        "\n",
        "    print(\"% of OOV words (not found in embedding): \", round(1.0 - words_found/matrix_len, 3) * 100)\n",
        "    print(\"50 OOV words: \", textwrap.fill(str(oov_words[:50]), width=100))\n",
        "    print(\"##############################################################\")\n",
        "\n",
        "    return torch.tensor(weights_matrix)\n",
        "\n",
        "def create_emb_layer(weights_matrix, trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "    # set trainable for weights\n",
        "    emb_layer.weight.requires_grad = trainable\n",
        "      \n",
        "    return emb_layer, num_embeddings, embedding_dim  \n",
        "\n",
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, target_vocab, embeddings_dict, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Learn new word embedding\n",
        "        # vocab_size = len(target_vocab)\n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Use pretrained word embedding\n",
        "        weights_matrix = get_weights(target_vocab, embeddings_dict, embedding_dim)\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, trainable=True)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        self.lstm.flatten_parameters() \n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # out = self.dropout(lstm_out)\n",
        "        out = self.fc1(lstm_out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMD5MO7bstul"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9irSayVjR6q"
      },
      "source": [
        "# Pad/truncate sentence to fixed length\n",
        "# https://arxiv.org/abs/1903.07288 pad zeros in front\n",
        "def pad_input(sentences, seq_len=100):\n",
        "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
        "    for i, tokens in enumerate(sentences):\n",
        "        if len(tokens) > seq_len:\n",
        "            features[i] = np.array(tokens)[:seq_len]\n",
        "        elif len(tokens) > 0:\n",
        "            features[i, -len(tokens):] = np.array(tokens)\n",
        "    return features\n",
        "\n",
        "def preprocess_input(X, y, seq_len=100, X_test_local=[], y_test_local=[]):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\n",
        "\n",
        "    \n",
        "    if len(X_test_local) > 0:\n",
        "        X_test = X_test_local\n",
        "        y_test = y_test_local\n",
        "\n",
        "    print(len(X_test))\n",
        "    words = Counter() \n",
        "\n",
        "    for i, sentence in enumerate(X_train):\n",
        "        # The sentences will be stored as a list of words/tokens\n",
        "        X_train[i] = []\n",
        "        for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
        "            words.update([word.lower()])  # Converting all the words to lowercase\n",
        "            X_train[i].append(word)\n",
        "    \n",
        "    words = {k:v for k,v in words.items() if v > 1}\n",
        "    words = sorted(words, key=words.get, reverse=True)\n",
        "    # Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
        "    words = ['_PAD', '_UNKNOWN'] + words\n",
        "\n",
        "    # Dictionaries to store the word to index mappings and vice versa\n",
        "    word2idx = {o:i for i,o in enumerate(words)}\n",
        "    idx2word = {i:o for i,o in enumerate(words)}\n",
        "\n",
        "    for i, sentence in enumerate(X_train):\n",
        "        X_train[i] = [word2idx[word] if word in word2idx else 1 for word in sentence]\n",
        "\n",
        "    for i, sentence in enumerate(X_val):\n",
        "        X_val[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\n",
        "\n",
        "    for i, sentence in enumerate(X_test):\n",
        "        X_test[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]\n",
        "\n",
        "    print(\"Training input statistics\")\n",
        "    print(\"Max tokens: \", np.max([len(x) for x in X_train]))\n",
        "    print(\"Min tokens: \", np.min([len(x) for x in X_train]))\n",
        "    print(\"Average tokens: \", np.mean([len(x) for x in X_train]))\n",
        "\n",
        "    train_sentences = pad_input(X_train, seq_len)\n",
        "    val_sentences = pad_input(X_val, seq_len)\n",
        "    test_sentences = pad_input(X_test, seq_len)\n",
        "\n",
        "    return train_sentences, val_sentences, test_sentences, y_train, y_val, y_test, words, idx2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6jR6yjifjbt"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "xzJddAwQfhSg",
        "outputId": "4482a206-797d-4663-de08-3ec72e95bfc7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize  \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\n",
        "train = pd.read_csv(url)\n",
        "X = np.array(train['comment_text'])\n",
        "y = np.array(train['toxic_label'])\n",
        "\n",
        "for i, sentence in enumerate(X):\n",
        "    X[i] = []\n",
        "    for word in nltk.word_tokenize(sentence): \n",
        "        X[i].append(word)\n",
        "\n",
        "x1 = [\"Toxic\", \"Non-toxic\"]\n",
        "y1 = [sum(y), len(y)-sum(y)]\n",
        "\n",
        "fig = plt.figure(figsize =(4,6))\n",
        "plt.bar(x1, y1, label=\"Blue Bar\", )\n",
        "plt.plot()\n",
        "\n",
        "plt.xlabel(\"Comment Type\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.title(\"Toxic/Non-toxic Distribution\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "n = [len(x) for x in X]\n",
        "m = [m for m in range(len(n))]\n",
        "\n",
        "plt.hist(n, bins=20, range=(0, 500))\n",
        "plt.title(\"Histogram\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qs3MAd1szWS"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pluu00U1PXCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f848415-6725-4ba9-fcca-35a0c46ce38c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize  \n",
        "\n",
        "\n",
        "def train_and_evaluate_model(embeddings_dict, X_test_local=[], y_test_local=[]):\n",
        "    url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\n",
        "    train = pd.read_csv(url)\n",
        "    X = np.array(train['comment_text'])\n",
        "    y = np.array(train['toxic_label'])\n",
        "\n",
        "    # reduce size for faster training\n",
        "    size = X.shape[0]\n",
        "    # size = 5000\n",
        "\n",
        "    X = X[:size]\n",
        "    y = y[:size]\n",
        "\n",
        "    train_sentences, val_sentences, test_sentences, \\\n",
        "    y_train, y_val, y_test, vocab, idx2word = preprocess_input(X, y, seq_len=200, X_test_local=X_test_local, y_test_local=y_test_local)\n",
        "\n",
        "    train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(y_train))\n",
        "    val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(y_val))\n",
        "    test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(y_test))\n",
        "\n",
        "    batch_size = 256\n",
        "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "    val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "    output_size = 1\n",
        "    embedding_dim = len(list(embeddings_dict.values())[0])\n",
        "    hidden_dim = 256\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    # print(device)\n",
        "\n",
        "    model = SentimentNet(vocab, embeddings_dict, output_size, embedding_dim, hidden_dim, n_layers=2, drop_prob=0.2)\n",
        "    model.to(device)\n",
        "\n",
        "    lr=0.0003\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n",
        "\n",
        "    epochs = 8\n",
        "    counter = 0\n",
        "    print_every = int(0.5 * len(y_train) / batch_size)\n",
        "    clip = 5\n",
        "    # valid_loss_min = np.Inf\n",
        "    valid_loss_min = 0.25\n",
        "    best_model_so_far = None\n",
        "\n",
        "    # Set seed\n",
        "    torch.manual_seed(1)\n",
        "    np.random.seed(0)\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "        h = model.init_hidden(batch_size)\n",
        "        \n",
        "        for inputs, labels in train_loader:\n",
        "            counter += 1\n",
        "            h = tuple([e.data for e in h])\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            model.zero_grad()\n",
        "            output, h = model(inputs, h)\n",
        "            loss = criterion(output.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "            # lr_scheduler.step()\n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "                val_h = model.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                model.eval()\n",
        "                for val_input, val_label in val_loader:\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    val_input, val_label = val_input.to(device), val_label.to(device)\n",
        "                    out, val_h = model(val_input, val_h)\n",
        "                    val_loss = criterion(out.squeeze(), val_label.float())\n",
        "                    val_losses.append(val_loss.item())\n",
        "                    \n",
        "                model.train()\n",
        "                print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "                if np.mean(val_losses) <= valid_loss_min:\n",
        "                    # torch.save(model.state_dict(), './state_dict.pt')\n",
        "                    best_model_so_far = copy.deepcopy(model)\n",
        "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                    valid_loss_min = np.mean(val_losses)\n",
        "\n",
        "    test_losses = []\n",
        "    h = model.init_hidden(batch_size)\n",
        "    test_inputs = np.array([])\n",
        "    outputs = np.array([])\n",
        "    test_labels = np.array([])\n",
        "\n",
        "    # load the best model which was saved previously\n",
        "    model =  best_model_so_far\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            h = tuple([each.data for each in h])\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            output, h = model(inputs, h)\n",
        "            test_loss = criterion(output.squeeze(), labels.float())\n",
        "            test_losses.append(test_loss.item())\n",
        "            pred = torch.round(output.squeeze())\n",
        "\n",
        "            if len(test_inputs) == 0:\n",
        "                test_inputs = inputs.cpu().numpy()\n",
        "            else:\n",
        "                test_inputs = np.concatenate([test_inputs, inputs.cpu().numpy()])\n",
        "            outputs = np.concatenate([outputs, pred.cpu().numpy()])\n",
        "            test_labels = np.concatenate([test_labels, labels.cpu().numpy()])\n",
        "        \n",
        "        f1score = f1_score(np.array(test_labels), outputs)\n",
        "        recall = recall_score(np.array(test_labels), outputs)\n",
        "        precision = precision_score(np.array(test_labels), outputs)\n",
        "        accuracy = accuracy_score(np.array(test_labels), outputs)\n",
        "        print(\"##############################################################\")\n",
        "        print(\"Test Statistics:\")\n",
        "        print(f\"Test F1 score: {f1score}\")\n",
        "        print(f\"Test recall: {recall}\")\n",
        "        print(f\"Test precision: {precision}\")\n",
        "        print(f\"Test accuracy: {accuracy}\")\n",
        "        print(\"##############################################################\")\n",
        "        # Print some wrong predictions\n",
        "        for i, input in enumerate(test_inputs[:100]):\n",
        "            if outputs[i] != test_labels[i]:\n",
        "              input_no_pad = input[input != 0]\n",
        "              print(f\"Label: {test_labels[i]}, Prediction: {outputs[i]}\")\n",
        "              print(\" \".join([idx2word[i] for i in input_no_pad]))\n",
        "              print()\n",
        "        return f1score, recall, precision, accuracy\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Wpx-5n3wRj"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3NrmQJuCbcy",
        "outputId": "3703c540-cc24-44bb-c8a3-1aacdf3b9be8"
      },
      "source": [
        "train_and_evaluate_model(embeddings_dict=twitter_embeddings_dict)\n",
        "train_and_evaluate_model(embeddings_dict=wikipedia_embeddings_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training input statistics\n",
            "Max tokens:  4948\n",
            "Min tokens:  2\n",
            "Average tokens:  75.06390318054257\n",
            "% of OOV words (not found in embedding):  25.5\n",
            "50 OOV words:  ['_PAD', '_UNKNOWN', '...', '2', '==', '1', '3', 'fucksex', 'yourselfgo', '====', 'buttsecks',\n",
            "'mothjer', '4', '5', '2005', 'contribs', 'pro-assad.hanibal911you', 'sexsex', 'rice=', '2006',\n",
            "\"'fuck\", '2007', '2008', '100', 'npov', 'admin-', 'criminalwar', '10', 'wikiproject', '2004',\n",
            "'marcolfuck', '6', 'penis/////small', '24', '~~~~', '7', 'barnstar', 'tommy2010', 'wikipedians',\n",
            "'2009', 'youbollocks', 'fart.china', 'ancestryfuck-off-jewish', '20', '2010', 'style=', '8',\n",
            "'j.delanoy', 'deleted.this', 'ullmann']\n",
            "##############################################################\n",
            "Epoch: 1/8... Step: 66... Loss: 0.408832... Val Loss: 0.406218\n",
            "Epoch: 1/8... Step: 132... Loss: 0.397059... Val Loss: 0.365559\n",
            "Epoch: 2/8... Step: 198... Loss: 0.296281... Val Loss: 0.294739\n",
            "Epoch: 2/8... Step: 264... Loss: 0.307180... Val Loss: 0.261605\n",
            "Epoch: 3/8... Step: 330... Loss: 0.340721... Val Loss: 0.252084\n",
            "Epoch: 3/8... Step: 396... Loss: 0.251658... Val Loss: 0.223443\n",
            "Validation loss decreased (0.250000 --> 0.223443).  Saving model ...\n",
            "Epoch: 4/8... Step: 462... Loss: 0.289121... Val Loss: 0.215099\n",
            "Validation loss decreased (0.223443 --> 0.215099).  Saving model ...\n",
            "Epoch: 4/8... Step: 528... Loss: 0.208663... Val Loss: 0.214400\n",
            "Validation loss decreased (0.215099 --> 0.214400).  Saving model ...\n",
            "Epoch: 5/8... Step: 594... Loss: 0.205723... Val Loss: 0.200035\n",
            "Validation loss decreased (0.214400 --> 0.200035).  Saving model ...\n",
            "Epoch: 5/8... Step: 660... Loss: 0.247539... Val Loss: 0.199684\n",
            "Validation loss decreased (0.200035 --> 0.199684).  Saving model ...\n",
            "Epoch: 6/8... Step: 726... Loss: 0.217116... Val Loss: 0.205121\n",
            "Epoch: 6/8... Step: 792... Loss: 0.211268... Val Loss: 0.187712\n",
            "Validation loss decreased (0.199684 --> 0.187712).  Saving model ...\n",
            "Epoch: 7/8... Step: 858... Loss: 0.267471... Val Loss: 0.192293\n",
            "Epoch: 7/8... Step: 924... Loss: 0.210235... Val Loss: 0.189768\n",
            "Epoch: 8/8... Step: 990... Loss: 0.179295... Val Loss: 0.183634\n",
            "Validation loss decreased (0.187712 --> 0.183634).  Saving model ...\n",
            "Epoch: 8/8... Step: 1056... Loss: 0.174373... Val Loss: 0.186969\n",
            "##############################################################\n",
            "Test Statistics:\n",
            "Test F1 score: 0.9038332212508408\n",
            "Test recall: 0.8807339449541285\n",
            "Test precision: 0.9281767955801105\n",
            "Test accuracy: 0.93017578125\n",
            "##############################################################\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i guess it 's easy to dish out criticism but you ca n't take it . do n't blame me for the fact that coldplay is a collection of _UNKNOWN music _UNKNOWN , _UNKNOWN by a badly diseased western media . i know what makes for music quality and coldplay is certainly anything but .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "you do not deserve to be alive look over your shoulder from now on\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "quit censoring or get _UNKNOWN removing the porn scandal background is against wiki policies . ==\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you are not targeting the players you are targeting the club ! ! ! you wo n't change my mind about it . you just want me to give you attention . goodbye al\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "stop with the warnings ill just be back lol lol lol lol lol lol u started it _UNKNOWN\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` the `` '' _UNKNOWN '' '' had a name . rory fox to be exact . good to see laziness still exists . poor _UNKNOWN what a a _UNKNOWN ''\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i do n't think he 's gay , but he may be in a relationship with a guy . what 's the big deal ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` here 's something for you to report you have more poop in your pants . vandals are people who disrupt wikipedia , like people who get bent out of shape over comments made on talk pages for ip address , rather than focusing on something ... important . they write condescending comments and put images of exclamation points in them and avoid communicating with other people like they 're people , instead they use an impersonal language of _UNKNOWN warnings sent like notices from bill collectors ( who would , hypothetically also have poopie pants ) . you may feel like you 're being very noble by reporting this kind of comments to aiv , and that when someone communicates with you in a way that you 're not _UNKNOWN with or that makes you uncomfortable that it 's `` '' vandalism '' '' and `` '' personal attacks . '' '' but it 's really not , it 's just an emerging bureaucratic culture preventing wikipedia from becoming a true form of social networking and collaboration . anyway , go ahead , go nuts , pick on people different from you and have me banned . just remember\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i 'm sure you eat alot of tube steak\n",
            "\n",
            "Training input statistics\n",
            "Max tokens:  4948\n",
            "Min tokens:  2\n",
            "Average tokens:  75.06390318054257\n",
            "% of OOV words (not found in embedding):  20.8\n",
            "50 OOV words:  ['_PAD', '_UNKNOWN', '==', 'dickhead', '•', 'fucksex', 'yourselfgo', '====', 'cocksucker',\n",
            "'buttsecks', 'mothjer', 'noobs', 'contribs', 'pro-assad.hanibal911you', 'sexsex', 'rice=', \"'fuck\",\n",
            "'npov', 'admin-', 'criminalwar', 'wikiproject', 'marcolfuck', 'wikipedian', 'penis/////small',\n",
            "'~~~~', 'barnstar', 'fack', 'tommy2010', 'cuntbag', 'tildes', 'cocksucking', 'youbollocks',\n",
            "'fart.china', 'ancestryfuck-off-jewish', 'style=', 'j.delanoy', 'deleted.this', 'sockpuppet',\n",
            "'shitfuck', 'notrhbysouthbanof', 'fggt', 'lmao', '..', 'supertr0ll', 'bleachanhero', 'aidsaids',\n",
            "'edgar181', 'bitchmattythewhite', 'fool.what', '\\u200e']\n",
            "##############################################################\n",
            "Epoch: 1/8... Step: 66... Loss: 0.443575... Val Loss: 0.466229\n",
            "Epoch: 1/8... Step: 132... Loss: 0.331970... Val Loss: 0.346610\n",
            "Epoch: 2/8... Step: 198... Loss: 0.322275... Val Loss: 0.327881\n",
            "Epoch: 2/8... Step: 264... Loss: 0.316988... Val Loss: 0.284292\n",
            "Epoch: 3/8... Step: 330... Loss: 0.347168... Val Loss: 0.261952\n",
            "Epoch: 3/8... Step: 396... Loss: 0.284178... Val Loss: 0.254517\n",
            "Epoch: 4/8... Step: 462... Loss: 0.315037... Val Loss: 0.270288\n",
            "Epoch: 4/8... Step: 528... Loss: 0.231323... Val Loss: 0.267962\n",
            "Epoch: 5/8... Step: 594... Loss: 0.219908... Val Loss: 0.228833\n",
            "Validation loss decreased (0.250000 --> 0.228833).  Saving model ...\n",
            "Epoch: 5/8... Step: 660... Loss: 0.265688... Val Loss: 0.234754\n",
            "Epoch: 6/8... Step: 726... Loss: 0.186129... Val Loss: 0.230051\n",
            "Epoch: 6/8... Step: 792... Loss: 0.230990... Val Loss: 0.228712\n",
            "Validation loss decreased (0.228833 --> 0.228712).  Saving model ...\n",
            "Epoch: 7/8... Step: 858... Loss: 0.222003... Val Loss: 0.215524\n",
            "Validation loss decreased (0.228712 --> 0.215524).  Saving model ...\n",
            "Epoch: 7/8... Step: 924... Loss: 0.269796... Val Loss: 0.221136\n",
            "Epoch: 8/8... Step: 990... Loss: 0.217911... Val Loss: 0.205146\n",
            "Validation loss decreased (0.215524 --> 0.205146).  Saving model ...\n",
            "Epoch: 8/8... Step: 1056... Loss: 0.183158... Val Loss: 0.208044\n",
            "##############################################################\n",
            "Test Statistics:\n",
            "Test F1 score: 0.8947368421052632\n",
            "Test recall: 0.8800786369593709\n",
            "Test precision: 0.9098915989159891\n",
            "Test accuracy: 0.9228515625\n",
            "##############################################################\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i guess it 's easy to dish out criticism but you ca n't take it . do n't blame me for the fact that coldplay is a collection of _UNKNOWN music _UNKNOWN , _UNKNOWN by a badly diseased western media . i know what makes for music quality and coldplay is certainly anything but .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "you do not deserve to be alive look over your shoulder from now on\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` why in the fvck would you give a `` '' love '' '' sign to a _UNKNOWN sockpuppet ? ''\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` the `` '' _UNKNOWN '' '' had a name . rory fox to be exact . good to see laziness still exists . poor _UNKNOWN what a a _UNKNOWN ''\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "i do n't think he 's gay , but he may be in a relationship with a guy . what 's the big deal ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "`` here 's something for you to report you have more poop in your pants . vandals are people who disrupt wikipedia , like people who get bent out of shape over comments made on talk pages for ip address , rather than focusing on something ... important . they write condescending comments and put images of exclamation points in them and avoid communicating with other people like they 're people , instead they use an impersonal language of _UNKNOWN warnings sent like notices from bill collectors ( who would , hypothetically also have poopie pants ) . you may feel like you 're being very noble by reporting this kind of comments to aiv , and that when someone communicates with you in a way that you 're not _UNKNOWN with or that makes you uncomfortable that it 's `` '' vandalism '' '' and `` '' personal attacks . '' '' but it 's really not , it 's just an emerging bureaucratic culture preventing wikipedia from becoming a true form of social networking and collaboration . anyway , go ahead , go nuts , pick on people different from you and have me banned . just remember\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "`` ? ? capitalism . what 's wrong with the _UNKNOWN poop doll in a list of obama presidency _UNKNOWN items ? ( see : `` '' obama poop doll '' '' ) ``\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8947368421052632, 0.8800786369593709, 0.9098915989159891, 0.9228515625)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3dDlYa95mbT"
      },
      "source": [
        "# Evaluate Model on Local Data (w Singlish)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb9OrUKM3UiZ",
        "outputId": "f76e8f44-5b5e-4bbe-bb08-b7c96183d48d"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/local_data_301.csv\"\n",
        "test_local = pd.read_csv(url)\n",
        "X_test_local = np.array(test_local['Comment'])\n",
        "y_test_local = np.array(test_local['Insult'])\n",
        "train_and_evaluate_model(embeddings_dict=wikipedia_embeddings_dict, X_test_local=X_test_local, y_test_local=y_test_local)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n",
            "Training input statistics\n",
            "Max tokens:  4948\n",
            "Min tokens:  2\n",
            "Average tokens:  75.06390318054257\n",
            "% of OOV words (not found in embedding):  20.8\n",
            "50 OOV words:  ['_PAD', '_UNKNOWN', '==', 'dickhead', '•', 'fucksex', 'yourselfgo', '====', 'cocksucker',\n",
            "'buttsecks', 'mothjer', 'noobs', 'contribs', 'pro-assad.hanibal911you', 'sexsex', 'rice=', \"'fuck\",\n",
            "'npov', 'admin-', 'criminalwar', 'wikiproject', 'marcolfuck', 'wikipedian', 'penis/////small',\n",
            "'~~~~', 'barnstar', 'fack', 'tommy2010', 'cuntbag', 'tildes', 'cocksucking', 'youbollocks',\n",
            "'fart.china', 'ancestryfuck-off-jewish', 'style=', 'j.delanoy', 'deleted.this', 'sockpuppet',\n",
            "'shitfuck', 'notrhbysouthbanof', 'fggt', 'lmao', '..', 'supertr0ll', 'bleachanhero', 'aidsaids',\n",
            "'edgar181', 'bitchmattythewhite', 'fool.what', '\\u200e']\n",
            "##############################################################\n",
            "Epoch: 1/8... Step: 66... Loss: 0.441901... Val Loss: 0.457461\n",
            "Epoch: 1/8... Step: 132... Loss: 0.317558... Val Loss: 0.345733\n",
            "Epoch: 2/8... Step: 198... Loss: 0.315951... Val Loss: 0.330948\n",
            "Epoch: 2/8... Step: 264... Loss: 0.326723... Val Loss: 0.312847\n",
            "Epoch: 3/8... Step: 330... Loss: 0.323169... Val Loss: 0.258992\n",
            "Epoch: 3/8... Step: 396... Loss: 0.260505... Val Loss: 0.248417\n",
            "Validation loss decreased (0.250000 --> 0.248417).  Saving model ...\n",
            "Epoch: 4/8... Step: 462... Loss: 0.324363... Val Loss: 0.236456\n",
            "Validation loss decreased (0.248417 --> 0.236456).  Saving model ...\n",
            "Epoch: 4/8... Step: 528... Loss: 0.236533... Val Loss: 0.254139\n",
            "Epoch: 5/8... Step: 594... Loss: 0.224407... Val Loss: 0.227129\n",
            "Validation loss decreased (0.236456 --> 0.227129).  Saving model ...\n",
            "Epoch: 5/8... Step: 660... Loss: 0.259452... Val Loss: 0.227237\n",
            "Epoch: 6/8... Step: 726... Loss: 0.178283... Val Loss: 0.220751\n",
            "Validation loss decreased (0.227129 --> 0.220751).  Saving model ...\n",
            "Epoch: 6/8... Step: 792... Loss: 0.246706... Val Loss: 0.247145\n",
            "Epoch: 7/8... Step: 858... Loss: 0.224053... Val Loss: 0.212492\n",
            "Validation loss decreased (0.220751 --> 0.212492).  Saving model ...\n",
            "Epoch: 7/8... Step: 924... Loss: 0.250887... Val Loss: 0.214499\n",
            "Epoch: 8/8... Step: 990... Loss: 0.220983... Val Loss: 0.220673\n",
            "Epoch: 8/8... Step: 1056... Loss: 0.213137... Val Loss: 0.206493\n",
            "Validation loss decreased (0.212492 --> 0.206493).  Saving model ...\n",
            "##############################################################\n",
            "Test Statistics:\n",
            "Test F1 score: 0.6666666666666667\n",
            "Test recall: 0.6869565217391305\n",
            "Test precision: 0.6475409836065574\n",
            "Test accuracy: 0.69140625\n",
            "##############################################################\n",
            "Label: 0.0, Prediction: 1.0\n",
            "delivery fees is already a killer\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "still got room not bad _UNKNOWN la : ( ( 14 days to change for future life.. jia you ah ! ! !\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "free money for them if you don ’ t wear mask and had an attitude like a _UNKNOWN during _UNKNOWN measures\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "the problem with ppl .. have so much money until become so stupid ... good luck\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "zero hope . the monkeys will just go extinct with the _UNKNOWN population plan .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "you this _UNKNOWN fker still talk about _UNKNOWN when everybody else is emphasising internal transfer ft. how low can you go ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN lang using _UNKNOWN wifi typing nonsense again..\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "give chance _UNKNOWN . he has nothing else going on in his life _UNKNOWN .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "shameless _UNKNOWN . first , their _UNKNOWN has caused the foreign workers to be infected . now talk so big . foreign workers should have been provided adequate protection and facilities . why were they exposed to undue risk . _UNKNOWN are totally irresponsible .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "wow you guys are missing the big picture . all these comments here are so toxic and self entitled . truly third world citizens\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "if you want to be a vigilante , then go join the police . why are people so nasty .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "stay positive . hiv positive .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "they better don ’ t breed\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "there are many social truths , u know i know can _UNKNOWN . no need to articulate it clearly and make everybody unhappy .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "`` smoke helps kill virus `` then everyone from now on must smoke to kill _UNKNOWN virus .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "you come here i pay you 600 u bend over so i can shove a broom up ur arse : x\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "damn yo we still got posts going on about this height thing ...\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "_UNKNOWN are a bunch of whiners\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "hence , may i appeal for all of you to stay in singapore , and avoid bringing back _UNKNOWN back into singapore upon your return ? this is for the sake of every _UNKNOWN , pr , and foreigner who responsibly chose to remain in singapore .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "all these “ precious princesses “ _UNKNOWN schooled from young to think like that . should just get _UNKNOWN and dumped and move on .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "what 's with ladies and their obsession over bags anyway ?\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "coach red pill mentioned that women look at men and divide them into general 80 % of the men ( who got no chance in dating them ) and the top 20 % of men ( in terms of wealth , looks and fame ) . good to see these top 20 % men fall .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "_UNKNOWN , erection still retain power . you guys really can not make it\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "if you just go _UNKNOWN _UNKNOWN ' you 'll make more sense you know\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "can she just _UNKNOWN _UNKNOWN . already shortage of mask and people are helping sg . really _UNKNOWN .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "dun need to marry or court sg girls _UNKNOWN only . the world is so big ! sg only little red dot . _UNKNOWN .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "do us a favour take a long walk on a short pier bye you will not be missed\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "because they always tend to go for the `` exotic looking '' ones ( read - ugly ) , with extremely small eyes or exaggerated asian features . most asian guys wo n't even find them attractive anyway !\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "do n't waste time with all these idiot , let them go and fly kite .\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "women have no loyalty , women wont love you like your mother , dont put women on the _UNKNOWN dont put her number 1 , they dont care how much money you spent on her before and they will leave when there is no more benefit ( unless they have no other choice ) below video will wake you up\n",
            "\n",
            "Label: 1.0, Prediction: 0.0\n",
            "just nuke _UNKNOWN do it good . do n't be like japan so greedy ... just make sure all chinese in china back then was killed especially the leaders of that era ... no such problem will we have today .\n",
            "\n",
            "Label: 0.0, Prediction: 1.0\n",
            "this donkey trump has blood in his hands\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666667, 0.6869565217391305, 0.6475409836065574, 0.69140625)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zswsuw35uV1Q"
      },
      "source": [
        "# Train Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7XqXLaruOzh",
        "outputId": "0070b795-51a6-4c81-82d9-8e8d0a70f043"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize  \n",
        "\n",
        "\n",
        "def main():\n",
        "    url = \"https://raw.githubusercontent.com/calvincxz/CS4248_Project/main/train2.csv\"\n",
        "    train = pd.read_csv(url)\n",
        "    X = np.array(train['comment_text'])\n",
        "    y = np.array(train['toxic_label'])\n",
        "\n",
        "    size = 10000\n",
        "    X = X[:size]\n",
        "    y = y[:size]\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=0)\n",
        "\n",
        "    model = LogisticRegression(random_state=0, solver='sag', max_iter=200)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_tf_idf_matrix = vectorizer.fit_transform(X_train)\n",
        "    model.fit( X_train_tf_idf_matrix, y_train)\n",
        "\n",
        "    # test your model\n",
        "    vectorizer_val = TfidfVectorizer(vocabulary=vectorizer.get_feature_names())\n",
        "    # vectorizer_val = TfidfVectorizer(vocabulary=vocab)\n",
        "    X_val_tf_idf_matrix = vectorizer_val.fit_transform(X_val)\n",
        "\n",
        "    y_pred = model.predict(X_val_tf_idf_matrix)\n",
        "    score = f1_score(y_val, y_pred, average='macro')\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    print('F1 score on validation = {}'.format(score))\n",
        "    print('accuracy = {}'.format(acc))\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "F1 score on validation = 0.7162126068376068\n",
            "accuracy = 0.932\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}